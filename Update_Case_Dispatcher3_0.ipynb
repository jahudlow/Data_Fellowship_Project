{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjuxiMPEH6309M+qen/Huz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahudlow/Data_Fellowship_Project/blob/master/Update_Case_Dispatcher3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhKbvdvXWSZc",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "!pip install -U scikit-learn==0.21.2\n",
        "\n",
        "import gspread\n",
        "import json\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import psycopg2\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "\n",
        "from copy import deepcopy\n",
        "from datetime import datetime, date\n",
        "from google.colab import auth, drive\n",
        "from io import StringIO\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from time import time\n",
        "\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pd.set_option('chained_assignment', None)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Shared\\ drives\n",
        "\n",
        "os.chdir('/content/drive/Shared drives/Colab_Notebooks/')\n",
        "\n",
        "import SL_Access.db_connect as dc\n",
        "import SL_Access.gsheets as gs\n",
        "db_cred = 'SL_Access/database.ini'\n",
        "#gs_cred='SL_Access/creds.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XCZyGoH906M"
      },
      "source": [
        "## Parameters\n",
        "All of the parameters below can be modified for the current run of the Case Dispatcher. \n",
        "\n",
        "After adjusting parameters as needed, select 'Runtime' -> 'Run All' from the menu above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDsuhGjC9ywR"
      },
      "source": [
        "# 'country' and 'version' are used to access the Case Dispatcher Google Sheet,\n",
        "# and should match the text in the Sheet title after 'Case Dispatcer 3.0 - '\n",
        "country = 'Uganda'\n",
        "\n",
        "version = ' - test'\n",
        "\n",
        "\n",
        "# Weights of 'Priority Rating' components:\n",
        "Priority_Weights = {\n",
        "    'Eminence':\t0.2,\n",
        "    'Solvability':\t0.6,\n",
        "    'Strength_of_Case':\t0.2,\n",
        "}\n",
        "\n",
        "# Weights of 'Solvability' sub-components:\n",
        "Solvability_Weights = {\n",
        "    'Victim_Willing_to_Testify': 2,\n",
        "    'Bio_and_Location_of_Suspect': 2,\n",
        "    'Other_Suspect(s)_Arrested': 1,\n",
        "    'Police_Willing_to_Arrest': 1,\n",
        "    'Recency_of_Case': 4,\n",
        "    'Exploitation_Reported': 2,\n",
        "    'PV_Believes': 2,\n",
        "}\n",
        "\n",
        "# The Case Dispatcher gives higher priority to more recent cases. The weight \n",
        "# placed on recency can be adjusted below (e.g. when 'Discount_Coef' is set to \n",
        "# 0.01, there is a 1% reduction in priority for each day that pases since the \n",
        "# case was initiated when the 'Discount_Exp' is set to 1, and a faster reduction \n",
        "# when it is > 1):\n",
        "Recency_Vars = {\n",
        "    'Discount_Coef': 0.01,\n",
        "    'Discount_Exp': 1,\n",
        "}\n",
        "\n",
        "# Weights for different beliefs the PV holds about the suspect:\n",
        "PV_Believes = {\n",
        "    'pv_believes_definitely_trafficked_many': 1,\n",
        "    'pv_believes_trafficked_some': 0.9,\n",
        "    'pv_believes_suspect_trafficker': 0.5,\n",
        "    'pv_believes_not_a_trafficker': 0\n",
        "}\n",
        "\n",
        "\n",
        "# Weights for different types of exploitation:\n",
        "Exploitation_Type = {\n",
        "  'exploitation_forced_prostitution_exp': 0.5, \n",
        "  'exploitation_sexual_abuse_exp': 0.45, \n",
        "  'exploitation_physical_abuse_exp': 0.3, \n",
        "  'exploitation_debt_bondage_exp': 0.4, \n",
        "  'exploitation_forced_labor_exp': 0.5, \n",
        "  'exploitation_organ_removal_exp': 0.5, \n",
        "  'exploitation_forced_prostitution_occ':\t1, \n",
        "  'exploitation_sexual_abuse_occ':\t0.9, \n",
        "  'exploitation_physical_abuse_occ':\t0.6, \n",
        "  'exploitation_debt_bondage_occ':\t0.8, \n",
        "  'exploitation_forced_labor_occ':\t1, \n",
        "  'exploitation_organ_removal_occ':\t1, \n",
        "}\n",
        "\n",
        "# 'V_Multiplier' dictionary increases the 'solvability' of a case according to \n",
        "# the number of victims who are willing to testify:\n",
        "v_mult = {\n",
        "    0:0, 1:.5, 7:1\n",
        "    }\n",
        "\n",
        "for i in range(1, 6):\n",
        "  v_mult[i + 1] = v_mult[i] + (1 - v_mult[i]) * .5\n",
        "\n",
        "\n",
        "gscv = 'On' # Grid Search Cross Validation - Options: 'On', 'Off'\n",
        "cutoff_days = 90 \n",
        "#For doing GridsearchCV to find the best model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-KV4Lu08z_o"
      },
      "source": [
        "## Google Sheets Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql0IR8oFRu6Z",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import gspread_dataframe as gd\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "gs_name = 'Case Dispatcher 3.0 - ' + country + version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e46t0sgcRTC_",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "sheet_names = ['Suspects',\n",
        "               'Victims',\n",
        "               'Police',\n",
        "               'Closed_Sus',\n",
        "               'Closed_Vic',\n",
        "               'Closed_Pol',\n",
        "               'Cases']\n",
        "\n",
        "def get_gsheets(workbook_name, sheet_names):\n",
        "    \"\"\"Return a list of Google worksheets from the name of a Google Sheet.\"\"\"\n",
        "    gsheets = []\n",
        "    for name in sheet_names:\n",
        "        sht = gc.open(workbook_name).worksheet(name)\n",
        "        gsheets.append(sht)\n",
        "    return gsheets\n",
        "\n",
        "cdws = get_gsheets(gs_name, sheet_names)\n",
        "dfs = gs.get_dfs(cdws)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsSG6BG89Xy"
      },
      "source": [
        "## Searchlight Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW42chXRN1po",
        "cellView": "form",
        "outputId": "6e39ea57-8961-44ee-b446-42ba7d7c118c"
      },
      "source": [
        "#@title\n",
        "dbc = dc.DB_Conn(db_cred)\n",
        "\n",
        "db_cif = dbc.ex_query(\"SELECT cif.cif_number, \\\n",
        "                      cif.interview_date, \\\n",
        "                      cif.number_of_victims, \\\n",
        "                      cif.case_notes, \\\n",
        "                      cif.number_of_traffickers, \\\n",
        "                      cif.recruited_agency, \\\n",
        "                      cif.recruited_broker, \\\n",
        "                      cif.recruited_no, \\\n",
        "                      cif.how_recruited_promised_job, \\\n",
        "                      cif.how_recruited_married, \\\n",
        "                      cif.how_recruited_promised_marriage, \\\n",
        "                      cif.how_recruited_at_work, \\\n",
        "                      cif.how_recruited_at_school, \\\n",
        "                      cif.how_recruited_job_ad, \\\n",
        "                      cif.how_recruited_broker_online, \\\n",
        "                      cif.how_recruited_broker_approached, \\\n",
        "                      cif.how_recruited_broker_through_friends, \\\n",
        "                      cif.how_recruited_broker_through_family, \\\n",
        "                      cif.how_recruited_broker_called_pv, \\\n",
        "                      cif.travel_expenses_paid_themselves, \\\n",
        "                      cif.travel_expenses_paid_by_broker, \\\n",
        "                      cif.expected_earning, \\\n",
        "                      cif.purpose_for_leaving_education, \\\n",
        "                      cif.purpose_for_leaving_travel_tour, \\\n",
        "                      cif.purpose_for_leaving_marriage, \\\n",
        "                      cif.purpose_for_leaving_family, \\\n",
        "                      cif.purpose_for_leaving_medical, \\\n",
        "                      cif.purpose_for_leaving_job_hotel, \\\n",
        "                      cif.purpose_for_leaving_job_household, \\\n",
        "                      cif.planned_destination, \\\n",
        "                      cif.id_made_no, \\\n",
        "                      cif.id_made_real, \\\n",
        "                      cif.id_made_fake, \\\n",
        "                      cif.id_made_false_name, \\\n",
        "                      cif.id_made_other_false, \\\n",
        "                      cif.exploitation_forced_prostitution_occ, \\\n",
        "                      cif.exploitation_forced_labor_occ, \\\n",
        "                      cif.exploitation_physical_abuse_occ, \\\n",
        "                      cif.exploitation_sexual_abuse_occ, \\\n",
        "                      cif.exploitation_debt_bondage_occ, \\\n",
        "                      cif.exploitation_organ_removal_occ, \\\n",
        "                      cif.exploitation_forced_prostitution_exp, \\\n",
        "                      cif.exploitation_forced_labor_exp, \\\n",
        "                      cif.exploitation_physical_abuse_exp, \\\n",
        "                      cif.exploitation_sexual_abuse_exp, \\\n",
        "                      cif.exploitation_debt_bondage_exp, \\\n",
        "                      cif.exploitation_organ_removal_exp, \\\n",
        "                      cif.legal_action_taken, \\\n",
        "                      cif.total_blue_flags, \\\n",
        "                      cif.station_id, \\\n",
        "                      pb.cif_id, \\\n",
        "                      pb.person_id, \\\n",
        "                      pb.pb_number, \\\n",
        "                      dp.arrested, \\\n",
        "                      dp.pv_believes, \\\n",
        "                      dp.role, \\\n",
        "                      bs.station_name \\\n",
        "                      FROM public.dataentry_cifcommon cif \\\n",
        "                      INNER JOIN public.dataentry_personboxcommon pb \\\n",
        "                      ON cif.id = pb.cif_id \\\n",
        "                      INNER JOIN public.dataentry_person dp \\\n",
        "                      ON pb.person_id = dp.id \\\n",
        "                      INNER JOIN public.dataentry_borderstation bs \\\n",
        "                      ON bs.id = cif.station_id \\\n",
        "                      INNER JOIN public.dataentry_country c \\\n",
        "                      ON c.id = bs.operating_country_id \\\n",
        "                      WHERE c.name = '{}'\".format(country))\n",
        "\n",
        "db_sus = dbc.ex_query(\"SELECT cif.cif_number, \\\n",
        "                      dp.full_name, \\\n",
        "                      dp.phone_contact, \\\n",
        "                      dp.address_notes, \\\n",
        "                      dp.case_filed_against, \\\n",
        "                      dp.social_media, \\\n",
        "                      dp.arrested, \\\n",
        "                      pb.person_id \\\n",
        "                      FROM public.dataentry_cifcommon cif \\\n",
        "                      INNER JOIN public.dataentry_personboxcommon pb \\\n",
        "                      ON cif.id = pb.cif_id \\\n",
        "                      INNER JOIN public.dataentry_person dp \\\n",
        "                      ON pb.person_id = dp.id \\\n",
        "                      INNER JOIN public.dataentry_borderstation bs \\\n",
        "                      ON bs.id = cif.station_id \\\n",
        "                      INNER JOIN public.dataentry_country c \\\n",
        "                      ON c.id = bs.operating_country_id \\\n",
        "                      WHERE c.name = '{}'\".format(country))\n",
        "\n",
        "db_vics = dbc.ex_query(\"SELECT cif.cif_number, \\\n",
        "                      dp.full_name, \\\n",
        "                      dp.phone_contact, \\\n",
        "                      dp.address_notes, \\\n",
        "                      dp.social_media \\\n",
        "                      FROM public.dataentry_cifcommon cif \\\n",
        "                      INNER JOIN public.dataentry_person dp \\\n",
        "                      ON cif.main_pv_id = dp.id \\\n",
        "                      INNER JOIN public.dataentry_borderstation bs \\\n",
        "                      ON bs.id = cif.station_id \\\n",
        "                      INNER JOIN public.dataentry_country c \\\n",
        "                      ON c.id = bs.operating_country_id \\\n",
        "                      WHERE c.name = '{}'\".format(country))\n",
        "\n",
        "irf_case_notes = dbc.ex_query(\"SELECT irf_number, \\\n",
        "                            case_notes \\\n",
        "                            FROM public.dataentry_irfcommon;\")\n",
        "\n",
        "dbc.close_conn()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PostgreSQL connection is closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2awzIFPnPEkb",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def generate_narrative(db_cif):\n",
        "    \"\"\"For cases that are missing a narrative, generate one from selected columns.\"\"\"\n",
        "    #without_cn = db_cif[db_cif['case_notes'] == '']\n",
        "    without_cn = db_cif\n",
        "    without_cn['narrative_broker_recruited'] = np.where(\n",
        "        without_cn['recruited_broker'] == True, \n",
        "        'They were recruited by a broker. ', '')\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_promised_job'] == True, \n",
        "        'Recruited by job promise. ', '')\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_married'] == True,\n",
        "        'Recruited by marriage. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_promised_marriage'] == True,\n",
        "        'Recruited by marriage promise. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_at_work'] == True,\n",
        "        'Recruited at work. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_at_school'] == True,\n",
        "        'Recruited at school. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_job_ad'] == True,\n",
        "        'Recruited through job ad. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_broker_online'] == True,\n",
        "        'Recruited online. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_broker_approached'] == True,\n",
        "        'Recruited by broker approaching them. ' \\\n",
        "        + without_cn['narrative_recruited'], \n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_broker_approached'] == True,\n",
        "        'Recruited by broker approaching them. ' \\\n",
        "        + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_broker_through_friends'] == True,\n",
        "        'Recruited through friends. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_recruited'] = np.where(\n",
        "        without_cn['how_recruited_broker_through_family'] == True,\n",
        "        'Recruited through family. ' + without_cn['narrative_recruited'],\n",
        "        without_cn['narrative_recruited'])\n",
        "    without_cn['narrative_travel_expenses_paid_themselves'] = np.where(\n",
        "        without_cn['travel_expenses_paid_themselves'] == True,\n",
        "        'They paid the travel expenses themselves. ', '')\n",
        "    without_cn['narrative_travel_expenses_paid_by_broker'] = np.where(\n",
        "        without_cn['travel_expenses_paid_by_broker'] == True,\n",
        "        'The broker paid the travel expenses. ', '')\n",
        "    without_cn['narrative_expected_earnings'] = np.where(\n",
        "        without_cn['expected_earning'] != '',\n",
        "        'Broker said they would be earning ' \\\n",
        "        + without_cn['expected_earning'] + ' per month. ', '')\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_education'] == True,\n",
        "        'Left home for education. ', '')\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_travel_tour'] == True,\n",
        "        'Left home for travel or tour. ', without_cn['narrative_purpose'])\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_marriage'] == True,\n",
        "        'Left home for marriage. ', without_cn['narrative_purpose'])\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_family'] == True,\n",
        "        'Left home for family. ', without_cn['narrative_purpose'])\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_medical'] == True,\n",
        "        'Left home for medical reasons. ', without_cn['narrative_purpose'])\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_job_hotel'] == True,\n",
        "        'Left home for job at hotel. ', without_cn['narrative_purpose'])\n",
        "    without_cn['narrative_purpose'] = np.where(\n",
        "        without_cn['purpose_for_leaving_job_household'] == True,\n",
        "        'Left home for household job. ', without_cn['narrative_purpose'])\n",
        "    without_cn['narrative_destination'] = np.where(\n",
        "        without_cn['planned_destination'] != '',\n",
        "        'Planned destination: ' + without_cn['planned_destination'] + ' ', '')\n",
        "    without_cn['narrative_id'] = np.where(\n",
        "        without_cn['id_made_no'] == True,\n",
        "        'No ID made. ', '')\n",
        "    without_cn['narrative_id'] = np.where(\n",
        "        without_cn['id_made_real'] == True,\n",
        "        'Real ID made. ', without_cn['narrative_id'])\n",
        "    without_cn['narrative_id'] = np.where(\n",
        "        without_cn['id_made_fake'] == True,\n",
        "        'Fake ID made. ', without_cn['narrative_id'])\n",
        "    without_cn['narrative_id'] = np.where(\n",
        "        without_cn['id_made_false_name'] == True,\n",
        "        'ID with false name made. ', without_cn['narrative_id'])\n",
        "    without_cn['narrative_id'] = np.where(\n",
        "        without_cn['id_made_other_false'] == True,\n",
        "        'ID made with other false info. ', without_cn['narrative_id'])\n",
        "    without_cn['narrative_legal'] = np.where(\n",
        "        without_cn['legal_action_taken'].str.contains('yes'),\n",
        "        'Legal Case Filed. ', '')\n",
        "    without_cn['narrative_pv_believes'] = np.where(\n",
        "        without_cn['pv_believes'].str.contains('Definitely', regex=False),\n",
        "        'PV believes the suspect has definitely trafficked many. ', '')\n",
        "    without_cn['narrative_pv_believes'] = np.where(\n",
        "        without_cn['pv_believes'].str.contains('some', regex=False),\n",
        "        'PV believes the suspect has trafficked some. ', \n",
        "        without_cn['narrative_pv_believes'])\n",
        "    without_cn['narrative_pv_believes'] = np.where(\n",
        "        without_cn['pv_believes'].str.contains('Suspect', regex=False),\n",
        "        'PV suspects they are a trafficker. ', \n",
        "        without_cn['narrative_pv_believes'])\n",
        "    without_cn['narrative_pv_believes'] = np.where(\n",
        "        without_cn['pv_believes'].str.contains('Don', regex=False),\n",
        "        'PV does not believe the suspect is a trafficker. ', \n",
        "        without_cn['narrative_pv_believes'])\n",
        "    without_cn['narrative'] = without_cn['narrative_broker_recruited'].fillna(\n",
        "        '') + without_cn['narrative_recruited'].fillna('') + \\\n",
        "    without_cn['narrative_travel_expenses_paid_themselves'].fillna('') + \\\n",
        "    without_cn['narrative_travel_expenses_paid_by_broker'].fillna('') + \\\n",
        "    without_cn['narrative_expected_earnings'].fillna('') + \\\n",
        "    without_cn['narrative_purpose'].fillna('') + \\\n",
        "    without_cn['narrative_destination'].fillna('') + \\\n",
        "    without_cn['narrative_id'].fillna('') + \\\n",
        "    without_cn['narrative_legal'].fillna('') + \\\n",
        "    without_cn['narrative_pv_believes'].fillna('')\n",
        "    without_cn['case_notes'] = np.where(\n",
        "        without_cn['case_notes'] == '',\n",
        "        without_cn['narrative'],\n",
        "        without_cn['case_notes'])\n",
        "    without_cn['case_notes'] = without_cn['case_notes'].replace(\n",
        "        'nan', np.nan).fillna('')\n",
        "    without_cn = without_cn[\n",
        "        without_cn.columns[\n",
        "            ~without_cn.columns.str.contains(\"narrative\")]]\n",
        "    return without_cn\n",
        "\n",
        "def pre_proc(db_cif):\n",
        "    \"\"\"Generates suspect IDs and narratives.\"\"\"\n",
        "\n",
        "    soc_df = db_cif[(db_cif.role != 'Complainant') & (db_cif.role != 'Witness')]\n",
        "    soc_df = soc_df[~soc_df['pb_number'].isna()]\n",
        "    soc_df['suspect_id'] = soc_df['cif_number'].str.replace('.', '')\n",
        "    soc_df['pb_number'] = soc_df['pb_number'].astype(int)\n",
        "    soc_df['suspect_id'] = soc_df['suspect_id'].str[:-1] \\\n",
        "        + \".PB\" + soc_df['pb_number'].map(str)\n",
        "    #soc_df = soc_df.drop_duplicates(subset='suspect_id')\n",
        "    soc_df = generate_narrative(soc_df)\n",
        "\n",
        "    return soc_df\n",
        "\n",
        "\n",
        "def organize_uganda_dest(soc_df):\n",
        "    \"\"\"Clean and organize desitnation data so it is ready for feature union.\"\"\"\n",
        "    soc_df['planned_destination'] = soc_df['planned_destination'].str.replace(\n",
        "        r'[^\\w\\s]+', '')\n",
        "    soc_df['destination_gulf'] = np.where(\n",
        "        soc_df['planned_destination'].str.contains(\n",
        "            'Gulf|Kuwait|Dubai|UAE|Oman|Saudi|Iraq|Qatar|Bahrain'), \n",
        "            True, False)\n",
        "    \n",
        "    dest = ['Kampala',\n",
        "            'Kyegegwa',\n",
        "            'Nairobi',\n",
        "            'Kenya']\n",
        "\n",
        "    for d in dest:\n",
        "        soc_df['destination_' + str(d)] = np.where(\n",
        "            soc_df['planned_destination'].str.contains(d),\n",
        "            True, False)\n",
        "\n",
        "    #soc_df.pb_number = soc_df.pb_number.fillna(0)\n",
        "    #soc_df.pb_number = soc_df.pb_number.astype(int)\n",
        "    return soc_df\n",
        "\n",
        "def organize_dtypes(soc_df):\n",
        "    \"\"\"Assigns relevant data types to variables.\"\"\"\n",
        "    num_features = [\n",
        "        'total_blue_flags',\n",
        "        'number_of_traffickers',\n",
        "        'number_of_victims',\n",
        "        'expected_earning']\n",
        "\n",
        "    boolean_features = list(\n",
        "        set(list(soc_df.columns)) -\n",
        "        set(num_features) -\n",
        "        set(['suspect_id', 'interview_date', 'case_notes', 'cif_number']))\n",
        "    soc_df[boolean_features] = soc_df[boolean_features].astype(bool)\n",
        "    soc_df[num_features] = soc_df[num_features].fillna(0).astype(float)\n",
        "\n",
        "    return soc_df\n",
        "\n",
        "\n",
        "def remove_non_numeric(x):\n",
        "    try:\n",
        "        x = re.sub(\"[^0-9]\", 0, x)\n",
        "    except:\n",
        "        x = 0\n",
        "    return x\n",
        "\n",
        "\n",
        "def en_features(soc_df):\n",
        "    \"\"\"Engineer features for selected destinations Person Box variables.\"\"\"\n",
        "    soc_df = organize_uganda_dest(soc_df)\n",
        "    \n",
        "    soc_df['expected_earning'] = soc_df['expected_earning'].apply(\n",
        "        lambda x: remove_non_numeric(x))\n",
        "    #soc_df['expected_earning'] = soc_df['expected_earning'].astype(int)\n",
        "  \n",
        "    soc_df['number_of_victims'] = np.where(\n",
        "        soc_df['number_of_victims'].isna(), 1, soc_df['number_of_victims'])\n",
        "    \n",
        "    soc_df['pv_believes_definitely_trafficked_many'] = np.where(\n",
        "        soc_df['pv_believes'].str.contains('Definitely', regex=False),\n",
        "        True, False)\n",
        "    soc_df['pv_believes_trafficked_some'] = np.where(\n",
        "        soc_df['pv_believes'].str.contains('some', regex=False),\n",
        "        True, False)\n",
        "    soc_df['pv_believes_suspect_trafficker'] = np.where(\n",
        "        soc_df['pv_believes'].str.contains('Suspect', regex=False),\n",
        "        True, False)\n",
        "    soc_df['pv_believes_not_a_trafficker'] = np.where(\n",
        "        soc_df['pv_believes'].str.contains('Don', regex=False),\n",
        "        True, False)\n",
        "\n",
        "    soc_df = soc_df.drop(columns=[\n",
        "        'arrested',\n",
        "        'station_id',\n",
        "        'cif_id',\n",
        "        'pb_number',\n",
        "        'role',\n",
        "        'planned_destination',\n",
        "        'pv_believes',\n",
        "        'legal_action_taken',\n",
        "        'station_name',\n",
        "        ])\n",
        "\n",
        "    soc_df = organize_dtypes(soc_df)\n",
        "\n",
        "    soc_df = soc_df.loc[:, (soc_df != False).any(axis=0)]\n",
        "\n",
        "    return soc_df\n",
        "\n",
        "\n",
        "class TypeSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"This is a class for applying transformations based on data type.\"\"\"\n",
        "\n",
        "    def __init__(self, dtype):\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        assert isinstance(X, pd.DataFrame)\n",
        "        return X.select_dtypes(include=[self.dtype])\n",
        "\n",
        "\n",
        "def build_transformer():\n",
        "    transformer = Pipeline([\n",
        "        ('features', FeatureUnion(transformer_list=[\n",
        "            ('boolean', Pipeline([\n",
        "                ('selector', TypeSelector('bool')),\n",
        "            ])),\n",
        "\n",
        "            ('numericals', Pipeline([\n",
        "                ('selector', TypeSelector(np.number)),\n",
        "                ('scaler', StandardScaler()),\n",
        "            ]))\n",
        "        ], n_jobs=1)),\n",
        "    ])\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def remove_recent(df, cutoff_days):\n",
        "    \"\"\"Eliminates cases more recent than the cutoff date.\"\"\"\n",
        "    today = date.today()\n",
        "    today.strftime(\"%m/%d/%Y\")\n",
        "    df['Days'] = (today - df.loc[:, 'interview_date']) / np.timedelta64(1, 'D')\n",
        "    sub_df = df[(df['Days'] > cutoff_days) | (df['Arrest'] == True)]\n",
        "    return sub_df\n",
        "\n",
        "\n",
        "def train_test_val_split(sub_df, te_size=.2, val_size=.1):\n",
        "    \"\"\"Splits dataset into training, testing, and validation sets.\"\"\"\n",
        "    X = (sub_df.drop(columns=['Arrest',\n",
        "                              'cif_number',\n",
        "                              'Days',\n",
        "                              'interview_date',\n",
        "                              'suspect_id',\n",
        "                              'person_id',\n",
        "                              'case_notes']))\n",
        "    y = sub_df.Arrest\n",
        "    val_size = val_size / (1 - te_size)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                        y,\n",
        "                                                        test_size=te_size)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train,\n",
        "                                                                    y_train,\n",
        "                                                                    test_size=val_size)\n",
        "    \n",
        "    return X_train, X_validation, y_train, y_validation\n",
        "\n",
        "\n",
        "def get_cls_pipe(clf=RandomForestClassifier()):\n",
        "    \"\"\"Builds pipeline with transformer and classifier algorithm.\"\"\"\n",
        "    transformer = build_transformer()\n",
        "    cls_pipeline = Pipeline([\n",
        "        ('transformer', transformer),\n",
        "        ('clf', clf)\n",
        "    ])\n",
        "    return cls_pipeline\n",
        "\n",
        "\n",
        "def pipe_predict(cls_pipeline, X_train, y_train, X_validation):\n",
        "    \"\"\"Make predictions with classifier pipeline.\"\"\"\n",
        "    cls_pipeline.fit(X_train, y_train)\n",
        "    y_rf = cls_pipeline.predict_proba(X_validation)\n",
        "    return y_rf\n",
        "\n",
        "\n",
        "def do_gridsearch(cls_pipeline, X_train, y_train):\n",
        "    \"\"\"Conducts gridsearch cross validation on selected classifer.\"\"\"\n",
        "    search_space = [{'clf': [RandomForestClassifier()],\n",
        "                     'clf__bootstrap': [False, True],\n",
        "                     'clf__n_estimators': [10, 100],\n",
        "                     #'clf__max_depth': [5, 10, 20, 30, 40, 50, None],\n",
        "                     'clf__max_depth': [20, 30],\n",
        "                     #'clf__max_features': [0.5, 0.6, 0.7, 0.8, 1],\n",
        "                     'clf__max_features': [0.5, 0.6],\n",
        "                     'clf__class_weight': [\"balanced\", None]}]\n",
        "                     #'clf__class_weight': [\"balanced\", \n",
        "                     #                      \"balanced_subsample\", None]}]\n",
        "    grid_search = GridSearchCV(cls_pipeline,\n",
        "                               search_space,\n",
        "                               cv=4, n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "    print(\"Performing grid search...\")\n",
        "    print(\"parameters:\")\n",
        "    print(search_space)\n",
        "    t0 = time()\n",
        "    best_model = grid_search.fit(X_train, y_train)\n",
        "    print(\"done in %0.3fs\" % (time() - t0))\n",
        "    print()\n",
        "    best_parameters = best_model.best_estimator_.get_params()['clf']\n",
        "\n",
        "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "    print(\"Best parameters set:\")\n",
        "    print(best_parameters)\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def full_gridsearch_pipe(soc_df, cutoff_days=90):\n",
        "    sub_df = remove_recent(soc_df, cutoff_days)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_val_split(sub_df)\n",
        "    cls_pipeline = get_cls_pipe()\n",
        "    best_model = do_gridsearch(cls_pipeline, X_train, y_train)\n",
        "    x_cols = list(X_validation.columns)\n",
        "    return best_model, x_cols, X_validation\n",
        "\n",
        "\n",
        "def check_grid_search_cv(soc_df, gscv, cutoff_days):\n",
        "    \"\"\"Check to see if Grid Search CV is on, and if it is run Grid Search CV.\"\"\"\n",
        "    if gscv == 'On':\n",
        "        best_model, x_cols, X_Validation = full_gridsearch_pipe(soc_df, \n",
        "                                                                cutoff_days)\n",
        "    return best_model, x_cols, X_Validation\n",
        "\n",
        "\n",
        "def make_new_predictions(df, soc_model, x_cols):\n",
        "    \"\"\"Use existing classifier algorithm on new cases without recalculating \n",
        "    best fit.\"\"\"\n",
        "    X = df[df.columns & x_cols]\n",
        "    df['soc'] = soc_model.predict_proba(X)[:, 1]\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GetAttr:\n",
        "    \"\"\"This is a class which allows objects in it's subclasses to be indexed.\"\"\"\n",
        "    def __getitem__(cls, x):\n",
        "        return getattr(cls, x)\n",
        "\n",
        "\n",
        "class EntityGroup(GetAttr):\n",
        "    \"\"\"This is a class for Victims, Suspects, and Police entity groups with \n",
        "    corresponding sheets.\"\"\"\n",
        "    sheets = []\n",
        "\n",
        "    def __init__(self, uid, new_cases, active_gsheet, closed_gsheet, gsdfs):\n",
        "        EntityGroup.sheets.append(self)\n",
        "        self.uid = uid\n",
        "        self.new = new_cases\n",
        "        self.gsheet = gsdfs[active_gsheet]\n",
        "        self.closed = gsdfs[closed_gsheet]\n",
        "        self.active_name = active_gsheet\n",
        "        self.closed_name = closed_gsheet\n",
        "\n",
        "    @classmethod\n",
        "    def merge_addresses(cls, addr):\n",
        "        \"\"\"Adds relevant address data to new entity groups.\"\"\"\n",
        "        addr = addr\n",
        "        for sheet in cls.sheets:\n",
        "            #sheet.new.infer_objects\n",
        "            if 'address1_id' in sheet.new:\n",
        "                sheet.new['address1_id'] = sheet.new['address1_id'].fillna(0).astype(int)\n",
        "                sheet.new['address2_id'] = sheet.new['address2_id'].fillna(0).astype(int)\n",
        "                sheet.new = pd.merge(sheet.new, addr, how='left', \n",
        "                                     on='address2_id')\n",
        "                sheet.new['Address'] = sheet.new['address_2'].map(str) + \", \" \\\n",
        "                + sheet.new['address_1']\n",
        "\n",
        "    @classmethod\n",
        "    def subset_country(cls, scc_key, country):\n",
        "        \"\"\"Subset dataframe for a specific country.\"\"\"\n",
        "        for sheet in cls.sheets:\n",
        "            sheet.new['station_code'] = sheet.new[sheet.uid].str[:3]\n",
        "            sheet.new = pd.merge(sheet.new, scc_key, how='left', \n",
        "                                 on='station_code')\n",
        "            sheet.new = sheet.new[sheet.new.country_name == country]\n",
        "            sheet.new = sheet.new.iloc[:, 0:len(sheet.new.columns)-2]\n",
        "            sheet.new.drop_duplicates(subset=sheet.uid, inplace=True)\n",
        "\n",
        "    @classmethod\n",
        "    def set_case_id(cls):\n",
        "        \"\"\"Creates a Case ID from the form ID stored in the database.\"\"\"\n",
        "        for sheet in cls.sheets:\n",
        "            sheet.new.loc[:, 'Case_ID'] = sheet.new['Case_ID'].str.replace('.', \n",
        "                                                                           '')\n",
        "            sheet.new['Case_ID'] = sheet.new['Case_ID'].str[:-1]\n",
        "\n",
        "    @classmethod\n",
        "    def combine_sheets(cls):\n",
        "        \"\"\"Adds new cases to data already in the corresponding Google Sheet.\"\"\"\n",
        "        for sheet in cls.sheets:\n",
        "            sheet.newcopy = deepcopy(sheet.new)\n",
        "            sheet.newcopy = sheet.newcopy.reindex(\n",
        "                columns=sheet.new.columns.tolist() + list(sheet.gsheet.columns))\n",
        "            sheet.newcopy = sheet.newcopy.iloc[:, 7:len(sheet.newcopy.columns)]\n",
        "            sheet.active = pd.concat([sheet.gsheet, sheet.newcopy], sort=False)\n",
        "            sheet.active.drop_duplicates(subset=sheet.uid, inplace=True)\n",
        "\n",
        "    @classmethod\n",
        "    def move_closed(cls, soc_df):\n",
        "        \"\"\"Moves closed cases to closed sheet for each Entity Group instance.\"\"\"\n",
        "        for sheet in cls.sheets:\n",
        "            prev_closed = sheet.newcopy[sheet.newcopy[sheet.uid].isin(\n",
        "                soc_df[soc_df.Arrest==1].suspect_id)]\n",
        "            prev_closed['Case_Status'] = \"Closed: Already in Legal Cases Sheet\"\n",
        "            newly_closed = sheet.gsheet[sheet.gsheet['Date_Closed'].str.len() \\\n",
        "                                        > 1]\n",
        "            prev_closed = prev_closed[\n",
        "                ~prev_closed[sheet.uid].isin(sheet.closed[sheet.uid])]\n",
        "            original_cols = list(prev_closed.columns)\n",
        "            new_cols = [original_cols[i] + str(i) for i in range(\n",
        "                len(original_cols))]\n",
        "            sheet.closed.columns = new_cols\n",
        "            prev_closed.columns = new_cols\n",
        "            newly_closed.columns = new_cols\n",
        "            sheet.closed = pd.concat([sheet.closed, prev_closed, newly_closed])\n",
        "            sheet.closed.columns = original_cols\n",
        "            sheet.closed.drop_duplicates(subset=sheet.uid, inplace=True)\n",
        "            sheet.active = sheet.active[~sheet.active[sheet.uid].isin(\n",
        "                sheet.closed[sheet.uid])]\n",
        "\n",
        "    @classmethod\n",
        "    def move_other_closed(cls, suspects, police, victims):\n",
        "        \"\"\"Moves cases closed in other Entity Groups to closed sheets.\"\"\"\n",
        "        closed_suspects = suspects.active[\n",
        "            (suspects.active['Suspect_ID'].isin(police.closed['Suspect_ID'])) |\n",
        "            (~suspects.active['Case_ID'].isin(victims.active['Case_ID']))]\n",
        "        closed_police = police.active[\n",
        "            (police.active['Suspect_ID'].isin(suspects.closed['Suspect_ID'])) |\n",
        "            (~police.active['Case_ID'].isin(victims.active['Case_ID']))]\n",
        "        closed_victims = victims.active[\n",
        "            (~victims.active['Case_ID'].isin(police.active['Case_ID'])) |\n",
        "            (~victims.active['Case_ID'].isin(suspects.active['Case_ID']))]\n",
        "        orig_scols = list(suspects.closed.columns)\n",
        "        new_scols = [orig_scols[i] + str(i) for i in range(len(orig_scols))]\n",
        "        suspects.closed.columns = new_scols\n",
        "        closed_suspects.columns = new_scols\n",
        "        suspects.closed = pd.concat(\n",
        "            [suspects.closed, closed_suspects])\n",
        "        suspects.closed.columns = orig_scols\n",
        "        suspects.closed.drop_duplicates(subset='Suspect_ID')\n",
        "        orig_pcols = list(police.closed.columns)\n",
        "        new_pcols = [orig_pcols[i] + str(i) for i in range(len(orig_pcols))]\n",
        "        police.closed.columns = new_pcols\n",
        "        closed_police.columns = new_pcols\n",
        "        police.closed = pd.concat(\n",
        "            [police.closed, closed_police])\n",
        "        police.closed.columns = orig_pcols\n",
        "        police.closed.drop_duplicates(subset='Suspect_ID')\n",
        "        orig_vcols = list(victims.closed.columns)\n",
        "        new_vcols = [orig_vcols[i] + str(i) for i in range(len(orig_vcols))]\n",
        "        victims.closed.columns = new_vcols\n",
        "        closed_victims.columns = new_vcols\n",
        "        victims.closed = pd.concat(\n",
        "            [victims.closed, closed_victims])\n",
        "        victims.closed.columns = orig_vcols\n",
        "        victims.closed.drop_duplicates(subset='Victim_ID', inplace=True)\n",
        "        \n",
        "        for sheet in cls.sheets:\n",
        "            sheet.active = sheet.active[~sheet.active[sheet.uid].isin(\n",
        "                sheet.closed[sheet.uid])]\n",
        "            sheet.closed = sheet.closed[sheet.closed['Case_ID']!='']\n",
        "            sheet.closed = sheet.closed.loc[:,:'Date_Closed']\n",
        "\n",
        "    @classmethod\n",
        "    def update_gsheets(cls, gs_name, active_cases):\n",
        "        \"\"\"Update Google Sheets with new data.\"\"\"\n",
        "        for sheet in cls.sheets:\n",
        "            target_sheet = gc.open(gs_name).worksheet(sheet.active_name)\n",
        "            up_sheet = sheet.active.iloc[:,:len(sheet.active.columns)-1]\n",
        "            gd.set_with_dataframe(target_sheet, up_sheet)\n",
        "            target_sheet = gc.open(gs_name).worksheet(sheet.closed_name)\n",
        "            gd.set_with_dataframe(target_sheet, sheet.closed)\n",
        "        target_sheet = gc.open(gs_name).worksheet('Cases')\n",
        "        up_sheet = active_cases.iloc[:,:len(active_cases.columns)]\n",
        "        gd.set_with_dataframe(target_sheet, up_sheet)\n",
        "\n",
        "    @classmethod\n",
        "    def add_irf_notes(cls, irf_notes):\n",
        "        \"\"\"Update Google Sheets with new data.\"\"\"\n",
        "        for sheet in cls.sheets:\n",
        "          sheet.active = pd.merge(sheet.active, irf_notes, how='left', \n",
        "                                  left_on='Case_ID', right_on='irf_number')\n",
        "          sheet.active['IRF_Case_Notes'] = sheet.active['case_notes']\n",
        "          sheet.active = sheet.active.loc[:,:'Date_Closed']\n",
        "\n",
        "    @classmethod\n",
        "    def add_case_name_formula(cls):\n",
        "      for sheet in cls.sheets:\n",
        "        sheet.active.reset_index(inplace=True)\n",
        "        sheet.active = sheet.active.drop(columns='index')\n",
        "        for index, row in sheet.active.iterrows():\n",
        "          sheet.active.at[index, 'Case_Name'] = '=iferror(index(Cases!B:B,match(A{},Cases!A:A,0)),)'.format(index + 2)\n",
        "\n",
        "    new_gsheets = []\n",
        "\n",
        "def set_vic_id(new_victims):\n",
        "    \"\"\"Creates a unique ID for each victim from Case ID and subsets/renames \n",
        "    columns.\"\"\"\n",
        "    new_victims = new_victims[['cif_number',\n",
        "                               'full_name',\n",
        "                               'phone_contact',\n",
        "                               'address_notes',\n",
        "                               'social_media']]\n",
        "    new_victims.loc[:, 'Victim_ID'] = new_victims['cif_number']\n",
        "    replacements = {\n",
        "        'Victim_ID': {\n",
        "            r'(\\.1|A$)': '.V1', r'B$': '.V2', r'C$': '.V3', r'D$': '.V4', \n",
        "            r'E$': '.V5', r'F$': '.V6', r'G$': '.V7', r'H$': '.V8', \n",
        "            r'I$': '.V9', r'J$': '.V10'}\n",
        "    }\n",
        "    new_victims.replace(replacements, regex=True, inplace=True)\n",
        "    new_victims.sort_values('full_name', inplace=True)\n",
        "    new_victims = new_victims.drop_duplicates(subset='Victim_ID')\n",
        "    non_blanks = new_victims['full_name'] != \"\"\n",
        "    new_victims = new_victims[non_blanks]\n",
        "    vcols = ['Case_ID', 'Name',\n",
        "             'Phone Number(s)',\n",
        "             'Address',\n",
        "             'Socail Media',\n",
        "             'Victim_ID']\n",
        "    new_victims.columns = vcols\n",
        "    new_victims['Narrative'] = ''\n",
        "    return new_victims\n",
        "\n",
        "\n",
        "def set_sus_id(new_suspects, db_cif):\n",
        "    \"\"\"Creates a unique ID for each suspect from Case ID and subsets/renames \n",
        "    columns.\"\"\"\n",
        "    new_suspects = new_suspects[['person_id',\n",
        "                                 'full_name',\n",
        "                                 'phone_contact',\n",
        "                                 'address_notes',\n",
        "                                 'social_media']]\n",
        "    cif_ids = db_cif[['cif_number', 'person_id', 'pb_number', 'case_notes']]\n",
        "    new_suspects = pd.merge(new_suspects, cif_ids, how='outer', on='person_id', \n",
        "                            sort=True, suffixes=('x', 'y'), copy=True)\n",
        "    new_suspects.loc[:, 'pb_number'] = new_suspects['pb_number'].fillna(\n",
        "        0).astype(int)\n",
        "    new_suspects.loc[:, 'Suspect_ID'] = new_suspects.\\\n",
        "    loc[:,'cif_number'].str.replace('.', '')\n",
        "    new_suspects.loc[:, 'Suspect_ID'] = new_suspects. \\\n",
        "    loc[:, 'Suspect_ID'].str[:-1] + \".PB\" + new_suspects['pb_number'].map(str)\n",
        "    new_suspects = new_suspects.drop_duplicates(subset='Suspect_ID')\n",
        "    new_suspects = new_suspects[['cif_number', 'Suspect_ID', 'full_name',\n",
        "                                 'phone_contact', 'address_notes',\n",
        "                                 'social_media', 'case_notes']]\n",
        "    new_suspects.rename(columns={\n",
        "        'full_name': 'Name',\n",
        "        'phone_contact': 'Phone Number(s)',\n",
        "        'address_notes': 'Address',\n",
        "        'social_media': 'Social Media ID',\n",
        "        'cif_number': 'Case_ID',\n",
        "        'case_notes': 'Narrative',\n",
        "        }, inplace=True)\n",
        "    return new_suspects\n",
        "\n",
        "def save_results(best_model, X_validation):\n",
        "    \"\"\"Pickles model and column names and saves them for later use.\"\"\"\n",
        "    pickle.dump(best_model, open('Case Dispatcher 3.0/u21_rf_model.sav', 'wb'))\n",
        "    xcols = list(X_validation.columns)\n",
        "    with open('Case Dispatcher 3.0/X_cols.txt', 'w') as f:\n",
        "        for item in xcols:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "\n",
        "def sum_and_join_vic(x):\n",
        "    \"\"\"Aggregate count of victims willing to testify by Case ID.\"\"\"\n",
        "    return pd.Series(dict(count=x['count'].sum(),\n",
        "                          willing_to_testify=', '.join(\n",
        "                              x.astype(str)['willing_to_testify'])))\n",
        "\n",
        "\n",
        "def sum_and_join_sus(x):\n",
        "    \"\"\"Aggregate count of suspects located by Case ID.\"\"\"\n",
        "    return pd.Series(dict(count=x['count'].sum(),\n",
        "                          located=', '.join(x.astype(str)['located'])))\n",
        "\n",
        "\n",
        "def get_vics_willing_to_testify(victims):\n",
        "    \"\"\"Get subset of victims who have indicated they're willing to testify \n",
        "    against traffickers.\"\"\"\n",
        "    vics_willing = victims.loc[\n",
        "                               victims['Case_Status'] == \\\n",
        "                               \"Step Complete: Victim is willing to press charges\"]\n",
        "    if len(vics_willing) > 0:\n",
        "        vics_willing = vics_willing[['Case_ID', 'Name']]\n",
        "        vics_willing.rename(columns={\"Name\": \"willing_to_testify\"},inplace=True)\n",
        "        vics_willing['count'] = 1\n",
        "        vics_willing = vics_willing.groupby('Case_ID').apply(sum_and_join_vic)\n",
        "    else:\n",
        "        vics_willing['willing_to_testify'] = ''\n",
        "    return vics_willing\n",
        "\n",
        "\n",
        "def add_vic_names(target_sheet, vics_willing):\n",
        "    \"\"\"Add comma separated list of victims willing to testify to active police \n",
        "    or suspect sheet.\"\"\"\n",
        "    if len(vics_willing) > 0:\n",
        "        target_sheet = pd.merge(target_sheet, vics_willing, how='left', \n",
        "                                on='Case_ID')\n",
        "        target_sheet['Victims_Willing_to_Testify'] = \\\n",
        "        target_sheet['willing_to_testify'].fillna('')\n",
        "        target_sheet.drop(columns=['willing_to_testify', 'count'], inplace=True)\n",
        "    return target_sheet\n",
        "\n",
        "\n",
        "def get_sus_located(suspects):\n",
        "    \"\"\"Get subset of suspects who have been identified and located.\"\"\"\n",
        "    sus_located = suspects.loc[\n",
        "        suspects.Case_Status.str.contains(\"Step Complete\", na=False)]\n",
        "    if len(sus_located) > 0:\n",
        "        sus_located = sus_located[['Case_ID', 'Name']]\n",
        "        sus_located.rename(columns={\"Name\": \"located\"}, inplace=True)\n",
        "        sus_located['count'] = 1\n",
        "        sus_located = sus_located.groupby('Case_ID').apply(sum_and_join_sus)\n",
        "    return sus_located\n",
        "\n",
        "\n",
        "def add_sus_located(target_sheet, sus_located):\n",
        "    \"\"\"Add comma separated list of suspects identified and located to other \n",
        "    sheet.\"\"\"\n",
        "    if len(sus_located) > 0:\n",
        "        target_sheet = pd.merge(target_sheet, sus_located, how='left', \n",
        "                                on='Case_ID')\n",
        "        target_sheet['Suspects_Identified_and_Located'] = \\\n",
        "        target_sheet['located'].fillna('')\n",
        "        target_sheet.drop(columns=['located', 'count'], inplace=True)\n",
        "    return target_sheet\n",
        "\n",
        "\n",
        "def calc_vics_willing_scores(sus, vics_willing):\n",
        "    \"\"\"Calculate scores for number of victims willing to testify and add them \n",
        "    to suspect sheet.\"\"\"\n",
        "    if len(vics_willing)>0:\n",
        "        sus = pd.merge(suspects.active, vics_willing, how='left', on='Case_ID')\n",
        "        sus['count'] = sus['count'].fillna(0).astype(int)\n",
        "        sus['V_Mult'] = sus['count'].map(v_mult)\n",
        "        sus.drop(columns=['willing_to_testify',\n",
        "                          'count'], inplace=True)\n",
        "        sus['V_Mult'].fillna(0, inplace=True)\n",
        "        sus['V_Mult'] = sus['V_Mult'].astype('float')\n",
        "    else:\n",
        "        sus['V_Mult'] = 0\n",
        "        sus['V_Mult'] = sus['V_Mult'].astype('float')\n",
        "    return sus\n",
        "\n",
        "\n",
        "def calc_arrest_scores(sus, soc_df, pol):\n",
        "    \"\"\"Calculate scores for the number of other suspects arrested in each case \n",
        "    and create fields for 'bio known' and for police willing to arrest.\"\"\"\n",
        "    sus['Bio_Known'] = np.where(\n",
        "        sus['Case_Status'].eq(\n",
        "            'Step Complete: Identity and Location Confirmed'), 0, 1)\n",
        "    arrests = get_total_arrests(soc_df)\n",
        "    sus = pd.merge(sus, arrests, how='left', on='Case_ID')\n",
        "    sus['Total_Arrests'] = sus['Total_Arrests'].fillna(0).astype(int)\n",
        "    sus.rename(columns={'Total_Arrests': 'Others_Arrested'}, inplace=True)\n",
        "    pol['Willing_to_Arrest'] = np.where(\n",
        "        pol.Case_Status.str.contains(\"Step Complete\", na=False), 1, 0)\n",
        "    sus = pd.merge(sus, pol[['Case_ID', 'Willing_to_Arrest']], how='left', \n",
        "                   on='Case_ID')\n",
        "    return sus\n",
        "\n",
        "\n",
        "def weight_pv_believes(sus, soc_df, PV_Believes):\n",
        "    pvb = soc_df[['cif_number',\n",
        "                  'pv_believes_definitely_trafficked_many',\n",
        "                  'pv_believes_trafficked_some',\n",
        "                  'pv_believes_suspect_trafficker']]\n",
        "    pvb['pv_believes'] = np.where(pvb[pvb.columns[1]]==True,\n",
        "                              PV_Believes[pvb.columns[1]],\n",
        "                              np.where(pvb[pvb.columns[2]]==True, \n",
        "                                       PV_Believes[pvb.columns[2]],\n",
        "                                       np.where(pvb[pvb.columns[3]]==True, \n",
        "                                                PV_Believes[pvb.columns[3]],0)))\n",
        "    pvb['Case_ID'] = pvb['cif_number'].str[:-1].replace('.', '')\n",
        "    pvb = pvb[['Case_ID', 'pv_believes']]\n",
        "    pvb['pv_believes'] = pvb['pv_believes'].astype(float)\n",
        "    pvb.drop_duplicates(subset='Case_ID', inplace=True)\n",
        "    sus = pd.merge(sus, pvb, how='left', on='Case_ID')\n",
        "    return sus\n",
        "\n",
        "\n",
        "def get_exp_score(sus, soc_df, Exploitation_Type):\n",
        "    \"\"\"Calculate exploitation score based on parameters and reported \n",
        "    exploitation.\"\"\"\n",
        "    exp_cols = [x for x in soc_df.columns if 'exploitation' in x]\n",
        "    exp_cols.append('cif_number')\n",
        "    exp_df = soc_df[exp_cols]\n",
        "    exp_df['exp'] = 0\n",
        "    for c in exp_df.columns:\n",
        "      try:\n",
        "        exp_df['exp'] = np.where(exp_df[c]==True,\n",
        "                                    Exploitation_Type[c] + exp_df['exp'],\n",
        "                                    exp_df['exp'])\n",
        "      except:\n",
        "        pass\n",
        "    exp_df['Case_ID'] = exp_df['cif_number'].str[:-1].replace('.', '')\n",
        "    exp_df = exp_df[['Case_ID', 'exp']]\n",
        "    exp_df['exp'] = exp_df['exp'].astype(float)\n",
        "    exp_df.drop_duplicates(subset='Case_ID', inplace=True)\n",
        "    sus = pd.merge(sus, exp_df, how='left', on='Case_ID')\n",
        "    return sus\n",
        "\n",
        "\n",
        "def calc_recency_scores(sus, soc_df, weights):\n",
        "    \"\"\"Assign score to each case that is higher the more recent it is.\"\"\"\n",
        "    today = date.today()\n",
        "    today.strftime(\"%m/%d/%Y\")\n",
        "    cif_dates = soc_df[['cif_number', 'interview_date']]\n",
        "    cif_dates['Days_Old'] = (\n",
        "        today - cif_dates.loc[:, 'interview_date']) / np.timedelta64(1, 'D')\n",
        "    cif_dates['Case_ID'] = cif_dates['cif_number'].str[:-1].replace('.', '')\n",
        "    sus = pd.merge(\n",
        "        sus, cif_dates[['Case_ID', 'Days_Old']], how='left', on='Case_ID')\n",
        "    coef = weights['Discount_Coef']\n",
        "    exp = weights['Discount_Exp']\n",
        "    sus['Recency_Score'] = np.where(\n",
        "        (1 - coef * sus['Days_Old'] ** exp) > 0, \n",
        "        1 - coef * sus['Days_Old'] ** exp, 0)\n",
        "    sus = sus.drop_duplicates(subset='Suspect_ID')\n",
        "    return sus\n",
        "\n",
        "\n",
        "def calc_network_scores(sus_with_links, sus):\n",
        "    \"\"\"\n",
        "    Calculate weighted scores based on 1st and 2nd degree links that each suspect\n",
        "    has with suspects from other cases and add these scores to the 'sus' dataframe.\n",
        "\n",
        "        1st degree case link = two suspects have a direct connection\n",
        "        2nd degree case link = two suspects are connected by one or more mutual contacts\n",
        "\n",
        "    The calculations are made by dividing the number of first and second degree case links\n",
        "    by the log (base 10) of the total number of connections the suspect has (plus nine).\n",
        "    Nine is added to the number of connections so that if the number of connections is\n",
        "    between 1-9 the product of the log will not be less than 1.\n",
        "    \"\"\"\n",
        "    sus_with_links['1d_case_score'] = sus_with_links['first_degree_case_links'] / np.log10(\n",
        "        sus_with_links['first_degree_links'] + 9)\n",
        "    sus_with_links['2d_case_score'] = sus_with_links['second_degree_case_links'] / np.log10(\n",
        "        sus_with_links['first_degree_links'] + 9)\n",
        "    sus = pd.merge(sus,\n",
        "                   sus_with_links[['suspect_case_id', '1d_case_score', '2d_case_score']],\n",
        "                   how='left', left_on='Suspect_ID', right_on='suspect_case_id')\n",
        "    sus.drop(columns=['suspect_case_id'], inplace=True)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def get_network_weights(Parameters):\n",
        "    \"\"\"Get weights for network analysis from Parameters Google Sheet.\"\"\"\n",
        "    net_weights = pd.DataFrame(Parameters.iloc[5:9, 4:6])\n",
        "    net_weights.columns = ['key', 'value']\n",
        "    return net_weights\n",
        "\n",
        "\n",
        "def weight_network_scores(sus, net_weights):\n",
        "    \"\"\"Weight the network scores according to the weights provided in the \n",
        "    Parameters Sheet.\"\"\"\n",
        "    s = net_weights.set_index('key')['value']\n",
        "    one_link_add = float(s['1 Link Em Added'])\n",
        "    max_add = float(s['Max Em Added'])\n",
        "    second_d_weight = float(s['2nd Degree Weight'])\n",
        "    sus['net_weight'] = sus['1d_case_score'] * one_link_add + (\n",
        "            sus['2d_case_score'] * one_link_add) * second_d_weight\n",
        "    sus['net_weight'].round(2)\n",
        "    sus['net_weight'] = np.where(\n",
        "        sus['net_weight'] > max_add,\n",
        "        max_add,\n",
        "        sus['net_weight'])\n",
        "    sus.drop(columns=['1d_case_score', '2d_case_score'], inplace=True)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def check_update_links(sus_with_links, sus, Parameters):\n",
        "    \"\"\"Check to see if Network Analysis is on, and if it is calculate network \n",
        "    scores and weight.\"\"\"\n",
        "    net_weights = get_network_weights(Parameters)\n",
        "    if net_weights.iloc[0,1] == 'On':\n",
        "        sus = calc_network_scores(sus_with_links, sus)\n",
        "        sus = weight_network_scores(sus, net_weights)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def get_eminence_score(sus):\n",
        "    \"\"\"Get eminence score from active sheet, if blank enter '1'.\"\"\"\n",
        "    sus['Em2'] = sus['Eminence'].fillna(1)\n",
        "    sus.loc[sus['Eminence'].str.len() < 1, 'Em2'] = 1\n",
        "    sus['Em2'] = sus['Em2'].astype(float)\n",
        "    if 'net_weight' in sus:\n",
        "        sus['Em2'] += sus['net_weight']\n",
        "        sus['Em2'] = np.where(sus['Em2'] > 9, 9, sus['Em2'])\n",
        "        sus['Em2'] = sus['Em2'].fillna(0)\n",
        "        sus.drop(columns=['net_weight'], inplace=True)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def get_sus_located_in(sus, location):\n",
        "    \"\"\"Get subset of suspects who have a particular location mentioned in \n",
        "    their address.\"\"\"\n",
        "    sus['loc'] = np.where(sus['Address'].str.contains(location), 1, 0)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def get_new_soc_score(sus, soc_df):\n",
        "    \"\"\"Merge newly calculated Strength of Case scores to suspects sheet.\"\"\"\n",
        "    sus = pd.merge(sus,\n",
        "                   soc_df[['suspect_id', 'soc']],\n",
        "                   how='left', left_on='Suspect_ID', right_on='suspect_id')\n",
        "    sus['Strength_of_Case'] = sus['soc'].round(decimals=3)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def calculate_weights(Parameters):\n",
        "    \"\"\"Get current weights from Parameters Google Sheet.\"\"\"\n",
        "    weights_vs = pd.Series(Parameters.iloc[0:16, 1]).replace('', 0).append(\n",
        "        pd.Series(Parameters.iloc[0:3, 5])).astype(float)\n",
        "    weights_keys = pd.Series(Parameters.iloc[0:16, 0]).append(\n",
        "        pd.Series(Parameters.iloc[0:3, 4]))\n",
        "    weights = {k: v for k, v in zip(weights_keys, weights_vs)}\n",
        "    return weights\n",
        "\n",
        "\n",
        "def calc_solvability(sus, weights):\n",
        "    \"\"\"Calculate weighted solvability score on active suspects.\"\"\"\n",
        "    sus['Solvability'] = (\n",
        "        sus['V_Mult'].apply(\n",
        "            lambda x: x * weights['Victim_Willing_to_Testify']).fillna(0) + \\\n",
        "        sus['Bio_Known'].apply(\n",
        "            lambda x: x * Solvability_Weights['Bio_and_Location_of_Suspect']).fillna(0) + \\\n",
        "        sus['Others_Arrested'].apply(\n",
        "          lambda x: x * weights['Other_Suspect(s)_Arrested']).fillna(0) + \\\n",
        "        sus['Willing_to_Arrest'].apply(\n",
        "          lambda x: x * weights['Police_Willing_to_Arrest']).fillna(0) + \\\n",
        "        sus['Recency_Score'].apply(\n",
        "          lambda x: x * weights['Recency_of_Case']).fillna(0) + \\\n",
        "        sus['pv_believes'].apply(\n",
        "          lambda x: x * weights['PV_Believes']).fillna(0) + \\\n",
        "        sus['exp'].apply(\n",
        "          lambda x: x * weights['Exploitation_Reported']).fillna(0)) \\\n",
        "          / sum(weights.values())\n",
        "    return sus\n",
        "\n",
        "\n",
        "def calc_priority(sus, weights, Suspects):\n",
        "    \"\"\"Calculate weighted priority score on active suspects.\"\"\"\n",
        "    sus['Priority'] = (\n",
        "        sus['Solvability'].apply(\n",
        "            lambda x: x * weights['Solvability']) + \\\n",
        "        sus['Strength_of_Case'].apply(\n",
        "            lambda x: x * weights['Strength_of_Case']) + \\\n",
        "        sus['Em2'].apply(\n",
        "            lambda x: x * 0.1 * weights['Eminence'])).round(decimals=3)\n",
        "    sus['Priority'] = sus['Priority'].fillna(0)\n",
        "    sus['Priority'].astype(float)\n",
        "    sus.sort_values('Priority', ascending=False, inplace=True)\n",
        "    sus = sus.iloc[:, 0:len(Suspects.columns)].fillna('')\n",
        "    sus = sus.drop_duplicates(subset='Suspect_ID')\n",
        "    return sus\n",
        "\n",
        "\n",
        "def truncate_rows(df, nrow=200):\n",
        "    df = df.iloc[:nrow, :]\n",
        "    return df\n",
        "\n",
        "\n",
        "def calc_all_sus_scores(sus, vics_willing, pol, db_cif, soc_df, \n",
        "                        Suspects):\n",
        "    \"\"\"Complete all suspect sheet calculations in priority_calc module.\"\"\"\n",
        "    sus = calc_vics_willing_scores(sus, vics_willing)\n",
        "    sus = calc_arrest_scores(sus, soc_df, pol)\n",
        "    weights = {\n",
        "        **Solvability_Weights, \n",
        "        **Recency_Vars, \n",
        "        **Exploitation_Type, \n",
        "        **PV_Believes, \n",
        "        }\n",
        "    sus = calc_recency_scores(sus, soc_df, weights)\n",
        "    sus = weight_pv_believes(sus, soc_df, PV_Believes)\n",
        "    sus = get_exp_score(sus, soc_df, Exploitation_Type)\n",
        "    sus = get_new_soc_score(sus, soc_df)\n",
        "    sus = get_eminence_score(sus)\n",
        "    sus = calc_solvability(sus, Solvability_Weights)\n",
        "    sus = calc_priority(sus, Priority_Weights, Suspects)\n",
        "    sus = truncate_rows(sus)\n",
        "    return sus\n",
        "\n",
        "\n",
        "def add_priority_to_others(sus, other_entity_group, id_type, entity_gsheet, \n",
        "                           uid):\n",
        "    \"\"\"Copy priority score from suspects to other active sheets and sort them \n",
        "    by priority.\"\"\"\n",
        "    other_entity_group = pd.merge(other_entity_group,\n",
        "                                  sus[[id_type, 'Priority', 'Narrative']],\n",
        "                                  how='outer', on=id_type)\n",
        "    other_entity_group = other_entity_group[other_entity_group['Priority']!='']\n",
        "    other_entity_group['Priority'] = other_entity_group['Priority'].fillna(\n",
        "        0).astype(float)\n",
        "    other_entity_group['Narrative_x'] = other_entity_group['Narrative_y']\n",
        "    other_entity_group.rename(columns={\n",
        "        'Narrative_x': 'Narrative'}, inplace=True)\n",
        "    other_entity_group.drop_duplicates(subset=uid, inplace=True)\n",
        "    other_entity_group.sort_values('Priority', ascending=False, inplace=True)\n",
        "    other_entity_group = other_entity_group.iloc[:, 0:len(\n",
        "        entity_gsheet.columns)].fillna('')\n",
        "    return other_entity_group\n",
        "\n",
        "\n",
        "def get_total_arrests(soc_df):\n",
        "    \"\"\"Create Case_ID from suspect_id and aggregate arrests.\"\"\"\n",
        "    arrests = deepcopy(soc_df)\n",
        "    arrests['Case_ID'] = arrests['suspect_id'].str.replace('.', '').str[:-3]\n",
        "    arrests = pd.pivot_table(arrests, values='Arrest', index='Case_ID', \n",
        "                             aggfunc='sum').reset_index()\n",
        "    arrests.columns = 'Case_ID', 'Total_Arrests'\n",
        "    return arrests\n",
        "\n",
        "\n",
        "def update_active_cases(active_suspects, active_police):\n",
        "    active_cases = active_suspects[['Case_ID', 'Case_Name', 'Priority', \n",
        "                                  'IRF_Case_Notes', 'Narrative', \n",
        "                                  'Case_Status']]\n",
        "    active_cases['Case_Status'] = np.where(active_cases.Case_ID.isin(\n",
        "        active_police[active_police.Case_Status.str.contains('Complete')]['Case_ID']),\n",
        "        \"Third Step Complete - Police are willing to arrest suspect.\",\n",
        "        np.where(active_cases.Case_ID.isin(\n",
        "            active_suspects[active_suspects.Case_Status.str.contains('Complete')]['Case_ID']),\n",
        "            \"Second Step Complete: Suspect Located\",\n",
        "            np.where(active_cases.Case_ID.isin(\n",
        "                active_suspects[active_suspects.Victims_Willing_to_Testify.str.contains(\",\")]['Case_ID']),\n",
        "                'First Step Complete: Two or more PVs willing to testify', \n",
        "                np.where(active_cases.Case_ID.isin(\n",
        "                    active_suspects[active_suspects.Victims_Willing_to_Testify!='']['Case_ID']),\n",
        "                    'First Step Complete: One PV willing to testify', ''))))\n",
        "    active_cases['Next_Action_Priority'] = np.where(active_cases.Case_ID.isin(\n",
        "        active_police[active_police.Case_Status.str.contains('Complete')]['Case_ID']),\n",
        "        \"Ensure Arrest is Made\",\n",
        "        np.where(active_cases.Case_ID.isin(\n",
        "            active_suspects[active_suspects.Case_Status.str.contains('Complete')]['Case_ID']),\n",
        "            \"Ask Police to Arrest\",\n",
        "            np.where(active_cases.Case_ID.isin(\n",
        "                active_suspects[active_suspects.Victims_Willing_to_Testify!='']['Case_ID']),\n",
        "               'Locate Suspect', 'Contact Victim')))\n",
        "    active_cases.drop_duplicates('Case_ID', inplace=True)\n",
        "    return active_cases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Case Dispatcher"
      ],
      "metadata": {
        "id": "FYVFe3X5HvGu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsfyEQKHTY9W",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "db_cif = pre_proc(db_cif)\n",
        "\n",
        "db_cif['Arrest'] = np.where(db_cif.arrested=='Yes', 1,0)\n",
        "\n",
        "soc_df = en_features(db_cif)\n",
        "\n",
        "with open('Case Dispatcher 3.0/u21_rf_model.sav', 'rb') as f:\n",
        "  best_model = pickle.load(f)\n",
        "\n",
        "x_cols = pd.read_table('Case Dispatcher 3.0/X_cols.txt', header=None)\n",
        "\n",
        "best_model, x_cols, X_Validation = check_grid_search_cv(soc_df, gscv, \n",
        "                                                        cutoff_days)\n",
        "save_results(best_model, X_Validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX1ATJ1BYFlv",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "soc_df = make_new_predictions(soc_df, best_model, x_cols)\n",
        "\n",
        "new_victims = db_vics\n",
        "victims = EntityGroup('Victim_ID',\n",
        "                              new_victims,\n",
        "                              'Victims',\n",
        "                              'Closed_Vic',\n",
        "                              dfs)\n",
        "new_suspects = db_sus\n",
        "suspects = EntityGroup('Suspect_ID',\n",
        "                               new_suspects,\n",
        "                               'Suspects',\n",
        "                               'Closed_Sus',\n",
        "                               dfs)\n",
        "\n",
        "victims.new = set_vic_id(victims.new)\n",
        "\n",
        "suspects.new = set_sus_id(suspects.new, db_cif)\n",
        "\n",
        "EntityGroup.set_case_id()\n",
        "\n",
        "new_police = deepcopy(x=suspects.new)\n",
        "new_police.rename(columns={'Name': 'Suspect_Name'}, inplace=True)\n",
        "police = EntityGroup('Suspect_ID',\n",
        "                     new_police,\n",
        "                     'Police',\n",
        "                     'Closed_Pol',\n",
        "                     dfs)\n",
        "\n",
        "EntityGroup.combine_sheets()\n",
        "\n",
        "EntityGroup.add_irf_notes(irf_case_notes)\n",
        "\n",
        "EntityGroup.move_closed(soc_df)\n",
        "\n",
        "EntityGroup.move_other_closed(suspects, police, victims)\n",
        "\n",
        "vics_willing = get_vics_willing_to_testify(victims.active)\n",
        "police.active = add_vic_names(police.active, vics_willing)\n",
        "suspects.active = add_vic_names(suspects.active, vics_willing)\n",
        "sus_located = get_sus_located(suspects.active)\n",
        "victims.active = add_sus_located(victims.active, sus_located)\n",
        "\n",
        "suspects.active = calc_all_sus_scores(suspects.active,\n",
        "                          vics_willing,\n",
        "                          police.active,\n",
        "                          db_cif,\n",
        "                          soc_df,\n",
        "                          dfs['Suspects'])\n",
        "\n",
        "victims.active = add_priority_to_others(suspects.active,\n",
        "                                        victims.active,\n",
        "                                        'Case_ID',\n",
        "                                        dfs['Victims'],\n",
        "                                        'Victim_ID')\n",
        "\n",
        "police.active = add_priority_to_others(suspects.active,\n",
        "                                              police.active,\n",
        "                                              'Suspect_ID',\n",
        "                                              dfs['Police'],\n",
        "                                              'Suspect_ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biv30AQmfHsU"
      },
      "source": [
        "active_cases = update_active_cases(suspects.active, police.active)\n",
        "EntityGroup.add_case_name_formula()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Case Dispatcher Google Sheet\n",
        "EntityGroup.update_gsheets(gs_name, active_cases)"
      ],
      "metadata": {
        "id": "M3kWCEFuuWq0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}