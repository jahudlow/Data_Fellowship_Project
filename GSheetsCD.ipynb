{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from config import config\n",
    "import db_module as dm\n",
    "import gsheets as gs\n",
    "from closed_date import add_cdate_var\n",
    "import gspread\n",
    "from oauth2client.client import SignedJwtAssertionCredentials\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import zipfile\n",
    "import re\n",
    "from datetime import date\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant tables from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_mudule.py\n",
    "#\n",
    "#from config import config\n",
    "#import pandas as pd\n",
    "#import psycopg2\n",
    "#\n",
    "#class DBConn(object):\n",
    "#    '''This is a class for querying the database and returning a pandas dataframe.'''\n",
    "#    def __init__(self):\n",
    "#        params = config()\n",
    "#        conn = psycopg2.connect(**params)\n",
    "#        self.cur = conn.cursor()\n",
    "#    def ex_query(self, query):\n",
    "#        query = query\n",
    "#        cur = self.cur\n",
    "#        cur.execute(query)\n",
    "#        colnames = [desc[0] for desc in cur.description]\n",
    "#        rows = cur.fetchall()\n",
    "#        cur.close()\n",
    "#        return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "cif = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM dataentry_cifnepal as CIF inner join \\\n",
    "    dataentry_personboxnepal as PB on CIF.id = PB.cif_id;\")\n",
    "db_cif = cif.ex_query(query)\n",
    "\n",
    "vics = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM public.dataentry_person as p inner join \\\n",
    "    dataentry_cifnepal as CIF on p.id = CIF.main_pv_id;\")\n",
    "db_vics = vics.ex_query(query)\n",
    "\n",
    "sus = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM public.dataentry_personboxnepal as pb inner join \\\n",
    "    public.dataentry_person as p on pb.person_id = p.id;\")\n",
    "db_sus = sus.ex_query(query)\n",
    "\n",
    "add = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM public.dataentry_address1 as ad1 inner join \\\n",
    "    public.dataentry_address2 as ad2 on ad1.id = ad2.address1_id;\")\n",
    "db_add = add.ex_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data from CIFs and create suspect and victim IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cif_dates = db_cif[['cif_number', 'interview_date']]\n",
    "\n",
    "cif_ids = db_cif[['cif_number', 'person_id', 'pb_number']]\n",
    "\n",
    "add = db_add.iloc[:,[1,6,7]]\n",
    "acols = ['address_1',\n",
    "         'address2_id',\n",
    "         'address_2']\n",
    "add.columns = acols\n",
    "\n",
    "db_vics.infer_objects\n",
    "db_vics['address1_id'] = db_vics['address1_id'].fillna(0).astype(int)\n",
    "db_vics['address2_id'] = db_vics['address2_id'].fillna(0).astype(int)\n",
    "db_vics = pd.merge(db_vics, add, how='left',on='address2_id')\n",
    "db_vics['Address'] = db_vics['address_2'].map(str) + \", \" + db_vics['address_1']\n",
    "cif_vics = db_vics[['cif_number',\n",
    "                    'full_name',\n",
    "                    'phone_contact',\n",
    "                    'Address']]\n",
    "\n",
    "db_sus.infer_objects\n",
    "db_sus['address1_id'] = db_sus['address1_id'].fillna(0).astype(int)\n",
    "db_sus['address2_id'] = db_sus['address2_id'].fillna(0).astype(int)\n",
    "db_sus = pd.merge(db_sus, add, how='left', on='address2_id')\n",
    "db_sus['Address'] = db_sus['address_2'].map(str) + \", \" + db_sus['address_1']\n",
    "db_sus = db_sus[['person_id', 'full_name', 'phone_contact', 'Address']]\n",
    "cif_sus = pd.merge(db_sus, cif_ids, how='outer', on='person_id', sort=True,\n",
    "         suffixes=('x', 'y'), copy=True)\n",
    "cif_sus.loc[:,'pb_number']=cif_sus['pb_number'].fillna(0).astype(int)\n",
    "cif_sus.loc[:,'Suspect_ID'] = cif_sus.loc[:,'cif_number'].str.replace('.','')\n",
    "cif_sus.loc[:,'Suspect_ID'] = cif_sus.loc[:,'Suspect_ID'].str[:-1] + \".PB\" + cif_sus['pb_number'].map(str)\n",
    "cif_sus = cif_sus.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "cif_vics.loc[:, 'Victim_ID'] = cif_vics['cif_number']\n",
    "replacements = {\n",
    "   'Victim_ID': {\n",
    "      r'(\\.1|A$)': '.V1',r'B$': '.V2',r'C$': '.V3', r'D$': '.V4',r'E$': '.V5',\n",
    "      r'F$': '.V6',r'G$': '.V7',r'H$': '.V8',r'I$': '.V9',r'J$': '.V10'}\n",
    "}\n",
    "cif_vics.replace(replacements, regex=True, inplace=True)\n",
    "cif_vics.sort_values('full_name',inplace=True)\n",
    "cif_vics = cif_vics.drop_duplicates(subset='Victim_ID')\n",
    "non_blanks = cif_vics['full_name'] != \"\"\n",
    "cif_vics = cif_vics[non_blanks]\n",
    "\n",
    "cif_sus.loc[:, 'cif_number'] = cif_sus['cif_number'].str.replace('.','')\n",
    "cif_vics['cif_number'] = cif_vics['cif_number'].str.replace('.','')\n",
    "cif_vics['cif_number'] = cif_vics['cif_number'].str[:-1]\n",
    "cif_sus['cif_number'] = cif_sus['cif_number'].str[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get current Case Dispatcher data from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsheets.py module (part 1):\n",
    "#\n",
    "# import gspread\n",
    "# import json\n",
    "# from oauth2client.client import SignedJwtAssertionCredentials\n",
    "# import pandas as pd\n",
    "# import re\n",
    "#\n",
    "# def get_gsheets(workbook_name):\n",
    "#    '''Return a list of Google worksheets from the name of a Google Sheet.'''\n",
    "#    json_key = json.load(open('creds.json'))\n",
    "#    scope = ['https://spreadsheets.google.com/feeds',\n",
    "#             'https://www.googleapis.com/auth/drive']\n",
    "#    credentials = SignedJwtAssertionCredentials(json_key['client_email'], json_key['private_key'].encode(), scope)\n",
    "#    file = gspread.authorize(credentials)  #remember to share new sheets with client email\n",
    "#    workbook = file.open(workbook_name)\n",
    "#    gsheets = workbook.worksheets()\n",
    "#    return gsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdws = gs.get_gsheets(\"Case Dispatcher 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsheets.py module (part 2):\n",
    "#\n",
    "#class GSheet:\n",
    "#    '''This is a class for Google Worksheets.'''\n",
    "#    def __init__(self, wrksht):\n",
    "#        self.wrksht = wrksht\n",
    "#        self.name = re.findall(r\"'(.*?)'\", str(wrksht))[0]\n",
    "#        df = pd.DataFrame(self.wrksht.get_all_values())\n",
    "#        df.columns = df.iloc[0]\n",
    "#        df.drop(0, inplace=True)\n",
    "#        self.df = df\n",
    "\n",
    "all_sheets = []\n",
    "for i in range(len(cdws)):\n",
    "        sheet = gs.GSheet(cdws[i])\n",
    "        all_sheets.append(sheet)\n",
    "\n",
    "dfs = {sheet.name: sheet.df for sheet in all_sheets}\n",
    "locals().update(dfs)\n",
    "\n",
    "\n",
    "# Tried to add the above functionality as a method in Gsheet class but it hasn't worked yet:\n",
    "#    def get_df(self):\n",
    "#        globals['{}'.format(self.name)] = self.df\n",
    "\n",
    "today = date.today().strftime(\"%m-%d-%Y\")\n",
    "zipname = today + \"_Backup_Sheets.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zipname, 'w') as csv_zip:\n",
    "    for k,v in dfs.items():\n",
    "        fname= k + \"_\" + today + \".csv\"\n",
    "        csv_zip.writestr(fname, pd.DataFrame(v).to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases from CIFs to CD data\n",
    "\n",
    "new_suspects = cif_sus.iloc[:,[1, 2, 3, 4, 6]]\n",
    "new_suspects.rename(columns = {\n",
    "    'full_name':'Name',\n",
    "    'phone_contact': 'Phone_Number(s)',\n",
    "    'cif_number': 'Case_ID'},\n",
    "           inplace=True)\n",
    "new_police = new_suspects\n",
    "new_suspects = new_suspects.reindex(columns=new_suspects.columns.tolist() + list(Suspects.columns))\n",
    "new_suspects = new_suspects.iloc[:,5:len(new_suspects.columns)]\n",
    "suspects = pd.concat([Suspects,new_suspects])\n",
    "suspects = suspects.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "new_victims = cif_vics\n",
    "vcols = ['Case_ID','Name',\n",
    "         'Phone_Number(s)',\n",
    "         'Address',\n",
    "         'Victim_ID']\n",
    "new_victims.columns = vcols\n",
    "new_victims = new_victims.reindex( columns = new_victims.columns.tolist() + list(Victims.columns))\n",
    "new_victims = new_victims.iloc[:,5:len(new_victims.columns)]\n",
    "victims = pd.concat([Victims,new_victims])\n",
    "victims = victims.drop_duplicates(subset='Victim_ID')\n",
    "\n",
    "new_police.rename(columns = {'Name': 'Suspect_Name'})\n",
    "new_police = new_police.reindex( columns = new_police.columns.tolist() + list(Police.columns))\n",
    "new_police = new_police.iloc[:,5:len(new_police.columns)]\n",
    "police = pd.concat([Police,new_police])\n",
    "police = police.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "## Organize Arrest data from Case Dispatcher\n",
    "\n",
    "arrests = pd.DataFrame(Arrests)\n",
    "arrests.infer_objects()\n",
    "arrests['Outcome (Arrest)'] = arrests['Outcome (Arrest)'].fillna(0).astype(int)\n",
    "arrests = arrests.loc[arrests['Outcome (Arrest)'] == 1]\n",
    "\n",
    "pbs = ['pb' + str(n) for n in range(1,8)]\n",
    "for pb in pbs:\n",
    "    arrests[pb + '_ID'] = arrests['IRF#'] + '.' + pb\n",
    "for pb in pbs:\n",
    "    arrests[pb + '_Case_ID'] = arrests['IRF#']\n",
    "\n",
    "dpb={}\n",
    "for pb in pbs:\n",
    "    cnames = [col for col in arrests.columns if pb in col]\n",
    "    dpb['df_{0}'.format(pb)]=pd.DataFrame(arrests[cnames])\n",
    "newcn = ['Name',\n",
    "         'Arrested',\n",
    "         'Arrest_Date',\n",
    "         'Suspect_ID',\n",
    "         'Case_ID']\n",
    "locals().update(dpb)\n",
    "\n",
    "df_list = [df_pb1,\n",
    "           df_pb2,\n",
    "           df_pb3,\n",
    "           df_pb4,\n",
    "           df_pb5,\n",
    "           df_pb6,\n",
    "           df_pb7]\n",
    "\n",
    "for i, df in enumerate(df_list, 1):\n",
    "    df.columns = ['Name',\n",
    "                  'Arrested',\n",
    "                  'Arrest_Date',\n",
    "                  'Suspect_ID',\n",
    "                  'Case_ID']\n",
    "\n",
    "df_pb_all = pd.concat(df_list)\n",
    "\n",
    "arrests = df_pb_all[df_pb_all.Arrested.str.contains(\"Yes\")]\n",
    "arrests['Total_Arrests'] = arrests.groupby(['Case_ID'])['Case_ID'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move closed cases to closed sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_closed_sus = suspects[suspects.Suspect_ID.isin(arrests.Suspect_ID)]\n",
    "prev_closed_pol = police[police.Suspect_ID.isin(arrests.Suspect_ID)]\n",
    "prev_closed_sus['Case_Status'] = \"Closed: Already in Legal Cases Sheet\"\n",
    "prev_closed_pol['Case_Status'] = \"Closed: Already in Legal Cases Sheet\"\n",
    "\n",
    "closed_suspects = suspects[suspects['Date_Closed'].str.len() > 1]\n",
    "closed_victims = victims[victims['Date_Closed'].str.len() > 1]\n",
    "closed_police = police[police['Date_Closed'].str.len() > 1]\n",
    "\n",
    "closed_cases = [closed_suspects,closed_victims,closed_police, prev_closed_sus, prev_closed_pol]\n",
    "\n",
    "# closed_date.py\n",
    "#\n",
    "#def add_cdate_var(Sheets):\n",
    "#    \"\"\"Adds a variable with the current date to the end of each dataframe in a list.\"\"\"\n",
    "#    today = date.today()\n",
    "#    for sheet in Sheets:\n",
    "#        if len(sheet)>0:\n",
    "#            sheet.loc[:,'Date_Closed'] = today.strftime(\"%m/%d/%Y\")\n",
    "#        else:\n",
    "#            sheet['Date_Closed'] = \"\"\n",
    "\n",
    "add_cdate_var(closed_cases)\n",
    "\n",
    "closed_sus = pd.concat([Closed_Sus,closed_suspects, prev_closed_sus], sort=False)\n",
    "closed_pol = pd.concat([Closed_Pol,closed_police, prev_closed_pol], sort=False)\n",
    "closed_vic = pd.concat([Closed_Vic,closed_victims], sort=False)\n",
    "\n",
    "# Next Step: Remove from Active Sheets\n",
    "suspects = suspects[~suspects.Suspect_ID.isin(closed_sus.Suspect_ID)]\n",
    "police = police[~police.Suspect_ID.isin(closed_pol.Suspect_ID)]\n",
    "victims = victims[~victims.Victim_ID.isin(closed_vic.Victim_ID)]\n",
    "\n",
    "closed_suspects = suspects[(suspects.Suspect_ID.isin(closed_pol.Suspect_ID)) |\n",
    "                            (~suspects.Case_ID.isin(victims.Case_ID))]\n",
    "closed_police = police[(police.Suspect_ID.isin(closed_sus.Suspect_ID)) |\n",
    "                            (~police.Case_ID.isin(victims.Case_ID))]\n",
    "closed_victims = victims[(~victims.Case_ID.isin(police.Case_ID)) |\n",
    "                           (~victims.Case_ID.isin(suspects.Case_ID))]\n",
    "\n",
    "closed_cases = [closed_suspects,\n",
    "                closed_victims,\n",
    "                closed_police]\n",
    "add_cdate_var(closed_cases)\n",
    "\n",
    "closed_sus = pd.concat([closed_sus,closed_suspects], sort=False).drop_duplicates(subset='Suspect_ID')\n",
    "closed_vic = pd.concat([closed_vic,closed_victims], sort=False).drop_duplicates(subset='Victim_ID')\n",
    "closed_pol = pd.concat([closed_pol,closed_police], sort=False).drop_duplicates(subset='Suspect_ID')\n",
    "closed_pol.drop(columns=['Victims_Willing_to_Testify'])\n",
    "\n",
    "suspects = suspects[~suspects.Suspect_ID.isin(closed_sus.Suspect_ID)]\n",
    "police = police[~police.Suspect_ID.isin(closed_pol.Suspect_ID)]\n",
    "victims = victims[~victims.Victim_ID.isin(closed_vic.Victim_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Case Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RandA\\Anaconda3\\envs\\py36_32\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "victims['willing_to_testify'] = victims.Name[victims.Case_Status.str.contains(\"Step Complete\", na=False)]\n",
    "vics_willing = victims[['Case_ID','willing_to_testify']]\n",
    "vics_willing = vics_willing.dropna(axis=0, subset=['willing_to_testify'])\n",
    "vics_willing['count'] = 1\n",
    "\n",
    "def sum_and_join(x):\n",
    "     return pd.Series(dict(count = x['count'].sum(), \n",
    "                        willing_to_testify = ', '.join(x.astype(str)['willing_to_testify'])))\n",
    "if len(vics_willing) > 0:\n",
    "    vics_Willing = vics_willing.groupby('Case_ID').apply(sum_and_join)\n",
    "    \n",
    "police = pd.merge(police, vics_willing, how='left',on='Case_ID')\n",
    "\n",
    "police.victims_willing_to_testify = police.willing_to_testify\n",
    "police.drop(columns=['willing_to_testify', 'count'], inplace=True)\n",
    "\n",
    "suspects = pd.merge(suspects, vics_willing, how='left',on='Case_ID')\n",
    "v_multiplier = pd.DataFrame(Parameters.iloc[:10,6:8])\n",
    "v_multiplier.Victims_Willing_to_Testify = v_multiplier.Victims_Willing_to_Testify.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects['count'] = suspects['count'].fillna(0).astype(int)\n",
    "suspects = pd.merge(suspects, v_multiplier,how='left',left_on='count',right_on='Victims_Willing_to_Testify')\n",
    "suspects.drop(columns=['Victims_Willing_to_Testify',\n",
    "                       'willing_to_testify',\n",
    "                       'count'], inplace=True)\n",
    "suspects['V_Multiplier'].fillna(0, inplace=True)\n",
    "suspects['V_Multiplier'] = suspects['V_Multiplier'].astype('float')\n",
    "suspects['Bio_Known'] = np.where(suspects['Bio_and_Location'].eq(''),0,1)\n",
    "suspects = pd.merge(suspects, arrests[['Case_ID','Total_Arrests']], how='left',on='Case_ID')\n",
    "suspects['Total_Arrests'] = suspects['Total_Arrests'].fillna(0).astype(int)\n",
    "suspects.rename(columns={'Total_Arrests':'Others_Arrested'}, inplace=True)\n",
    "police['Willing_to_Arrest'] = np.where(police.Case_Status.str.contains(\"Step Complete\", na=False),1,0)\n",
    "suspects = pd.merge(suspects,police[['Case_ID','Willing_to_Arrest']], how='left', on='Case_ID')\n",
    "\n",
    "today = date.today()\n",
    "today.strftime(\"%m/%d/%Y\")\n",
    "cif_dates['Case_ID'] = cif_dates['cif_number'].str[:-1].replace('.','')\n",
    "cif_dates['Days_Old'] = (today - cif_dates.loc[:,'interview_date']) / np.timedelta64(1, 'D')\n",
    "suspects = pd.merge(suspects,cif_dates[['Case_ID','Days_Old']], how='left', on='Case_ID')\n",
    "suspects['Recency_Score'] = np.where(suspects['Days_Old']<100, 1 - suspects.Days_Old * .01, 0)\n",
    "suspects = suspects.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "# Get 'Strength of Case' results of CD module\n",
    "soc = pd.read_csv(\"soc.csv\")\n",
    "suspects = pd.merge(suspects,soc,how='left',left_on='Suspect_ID',right_on='suspect_id')\n",
    "suspects['Strength_of_Case'] = suspects['SOC'].round(decimals = 3)\n",
    "suspects['Strength_of_Case']\n",
    "\n",
    "suspects['Em2'] = suspects['Eminence'].fillna(1)\n",
    "suspects.loc[suspects['Eminence'].str.len() < 1, 'Em2'] = 1\n",
    "suspects['Em2'] = suspects['Em2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_Vs = pd.Series(\n",
    "    Parameters.iloc[0:7,1]).replace('',0).append(\n",
    "    pd.Series(\n",
    "        Parameters.iloc[0:3,5])).astype(float)\n",
    "\n",
    "weights_Keys = pd.Series(\n",
    "    Parameters.iloc[0:7,0]).append(\n",
    "    pd.Series(\n",
    "        Parameters.iloc[0:3,4]))\n",
    "\n",
    "weights = {k:v for k,v in zip(weights_Keys, weights_Vs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects['Solvability'] = (\n",
    "    suspects['V_Multiplier'].apply(lambda x: x * weights['Victim Willing to Testify']) + \\\n",
    "    suspects['Bio_Known'].apply(lambda x: x * weights['Bio and Location of Suspect']) + \\\n",
    "    suspects['Others_Arrested'].apply(lambda x: x * weights['Other Suspect(s) Arrested']) + \\\n",
    "    suspects['Willing_to_Arrest'].apply(lambda x: x * weights['Police Willing to Arrest']) + \\\n",
    "    suspects['Recency_Score'].apply(lambda x: x * weights['Recency of Case'])\n",
    ")/sum(weights.values())\n",
    "\n",
    "suspects['Priority'] = (\n",
    "    suspects['Solvability'].apply(lambda x: x * weights['Solvability']) + \\\n",
    "    suspects['Strength_of_Case'].apply(lambda x: x * weights['Strength of Case']) + \\\n",
    "    suspects['Em2'].apply(lambda x: x * 0.1 * weights['Eminence'])\n",
    "    ).round(decimals = 3)\n",
    "suspects['Priority'] = suspects['Priority'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload results to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects['Priority'].astype(float)\n",
    "suspects.sort_values('Priority',ascending=False, inplace=True)\n",
    "suspects = suspects.iloc[:,0:len(Suspects.columns)].fillna('')\n",
    "\n",
    "police = pd.merge(police, suspects[['Suspect_ID','Priority']])\n",
    "police['Priority'].astype(float)\n",
    "police.sort_values('Priority',ascending=False, inplace=True)\n",
    "police = police.iloc[:,0:len(Police.columns)].fillna('')\n",
    "\n",
    "victims = pd.merge(victims, suspects[['Case_ID','Priority']])\n",
    "victims.sort_values('Priority',ascending=False, inplace=True)\n",
    "victims = victims.iloc[:,0:len(Victims.columns)].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects.name = 'suspects.csv'\n",
    "police.name = 'police.csv'\n",
    "victims.name = 'victims.csv'\n",
    "closed_sus.name = 'closed_sus.csv'\n",
    "closed_pol.name = 'closed_pol.csv'\n",
    "closed_vic.name = 'closed_vic.csv'\n",
    "\n",
    "all_sheets = [suspects,\n",
    "              police,\n",
    "              victims,\n",
    "              closed_sus,\n",
    "              closed_pol,\n",
    "              closed_vic]\n",
    "\n",
    "for sheet in all_sheets:\n",
    "   sheet.to_csv(sheet.name,index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_sheets = []\n",
    "\n",
    "for sheet in all_sheets:\n",
    "    up_sheet = open(sheet.name,'r').read()\n",
    "    up_sheets.append(up_sheet)\n",
    "    \n",
    "file = gspread.authorize(credentials)\n",
    "\n",
    "ss = file.open(\"Suspects\")\n",
    "ps = file.open(\"Police\")\n",
    "vs = file.open(\"Victims\")\n",
    "css = file.open(\"Closed_Sus\")\n",
    "cps = file.open(\"Closed_Pol\")\n",
    "cvs = file.open(\"Closed_Vic\")\n",
    "\n",
    "gs = [ss,\n",
    "      ps,\n",
    "      vs,\n",
    "      css,\n",
    "      cps,\n",
    "      cvs]\n",
    "\n",
    "sheet_dict = {k:v for k,v in zip(gs, up_sheets)}\n",
    "\n",
    "def upload_sheets(dict):\n",
    "    \"\"\"Uploads csv files to Google Sheets from a dictionary where keys are Google Sheets and values are csvs.\"\"\"\n",
    "    file = gspread.authorize(credentials)\n",
    "    last = len(list(dict))\n",
    "    for i in range(0,last):\n",
    "        file.import_csv(\n",
    "            list(\n",
    "                dict.keys())[i].id,\n",
    "            list(\n",
    "                dict.values())[i].encode('utf-8'))\n",
    "\n",
    "upload_sheets(sheet_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36_32)",
   "language": "python",
   "name": "py36_32"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
