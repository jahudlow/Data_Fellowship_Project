{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gspread\n",
    "from oauth2client.client import SignedJwtAssertionCredentials\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant tables from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from config import config\n",
    "\n",
    "params = config()\n",
    "conn = psycopg2.connect(**params)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    \"SELECT * FROM dataentry_cifnepal as CIF inner join dataentry_personboxnepal as PB on CIF.id = PB.cif_id;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "rows = cur.fetchall()\n",
    "dfcif = pd.DataFrame(rows, columns = colnames)\n",
    "cur.execute(\n",
    "    \"SELECT * FROM public.dataentry_person as p inner join dataentry_cifnepal as CIF on p.id = CIF.main_pv_id;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "rows = cur.fetchall()\n",
    "dfv = pd.DataFrame(rows, columns = colnames)\n",
    "cur.execute(\n",
    "    \"SELECT * FROM public.dataentry_personboxnepal as pb inner join public.dataentry_person as p on pb.person_id = p.id;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "rows = cur.fetchall()\n",
    "dfs = pd.DataFrame(rows, columns = colnames)\n",
    "cur.execute(\n",
    "    \"SELECT * FROM public.dataentry_address1 as ad1 inner join public.dataentry_address2 as ad2 on ad1.id = ad2.address1_id;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "rows = cur.fetchall()\n",
    "add = pd.DataFrame(rows, columns = colnames)\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data from CIFs and create suspect and victim IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcif = dfcif[['cif_number','person_id','pb_number']]\n",
    "\n",
    "add = add.iloc[:,[1,6,7]]\n",
    "acols = ['address_1','address2_id','address_2']\n",
    "add.columns = acols\n",
    "\n",
    "dfv.infer_objects\n",
    "dfv['address1_id'] = dfv['address1_id'].fillna(0).astype(int)\n",
    "dfv['address2_id'] = dfv['address2_id'].fillna(0).astype(int)\n",
    "dfv = pd.merge(dfv, add, how='left',on='address2_id')\n",
    "dfv['Address'] = dfv['address_2'].map(str) + \", \" + dfv['address_1']\n",
    "CIF_Victims = dfv[['cif_number','full_name','phone_contact','Address']]\n",
    "\n",
    "dfs.infer_objects\n",
    "dfs['address1_id'] = dfs['address1_id'].fillna(0).astype(int)\n",
    "dfs['address2_id'] = dfs['address2_id'].fillna(0).astype(int)\n",
    "dfs = pd.merge(dfs, add, how='left',on='address2_id')\n",
    "dfs['Address'] = dfs['address_2'].map(str) + \", \" + dfs['address_1']\n",
    "dfs = dfs[['person_id','full_name','phone_contact','Address']]\n",
    "CIF_Suspects = pd.merge(dfs, dfcif, how='outer',on='person_id', sort=True,\n",
    "         suffixes=('x', 'y'), copy=True)\n",
    "CIF_Suspects.loc[:,'pb_number']=CIF_Suspects['pb_number'].fillna(0).astype(int)\n",
    "CIF_Suspects.loc[:,'Suspect_ID'] = CIF_Suspects.loc[:,'cif_number'].str.replace('.','')\n",
    "CIF_Suspects.loc[:,'Suspect_ID'] = CIF_Suspects.loc[:,'Suspect_ID'].str[:-1] + \".PB\" + CIF_Suspects['pb_number'].map(str)\n",
    "CIF_Suspects = CIF_Suspects.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "CIF_Victims.loc[:,'Victim_ID'] = CIF_Victims['cif_number']\n",
    "replacements = {\n",
    "   'Victim_ID': {\n",
    "      r'(\\.1|A$)': '.V1',r'B$': '.V2',r'C$': '.V3', r'D$': '.V4',r'E$': '.V5',\n",
    "      r'F$': '.V6',r'G$': '.V7',r'H$': '.V8',r'I$': '.V9',r'J$': '.V10'}\n",
    "}\n",
    "CIF_Victims.replace(replacements, regex=True, inplace=True)\n",
    "CIF_Victims.sort_values('full_name',inplace=True)\n",
    "CIF_Victims = CIF_Victims.drop_duplicates(subset='Victim_ID')\n",
    "non_blanks = CIF_Victims['full_name'] != \"\"\n",
    "CIF_Victims = CIF_Victims[non_blanks]\n",
    "\n",
    "CIF_Suspects.loc[:,'cif_number'] = CIF_Suspects['cif_number'].str.replace('.','')\n",
    "CIF_Victims['cif_number'] = CIF_Victims['cif_number'].str.replace('.','')\n",
    "CIF_Victims['cif_number'] = CIF_Victims['cif_number'].str[:-1]\n",
    "CIF_Suspects['cif_number'] = CIF_Suspects['cif_number'].str[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get current Case Dispatcher data from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting latest CD data from Google Sheets\n",
    "\n",
    "json_key = json.load(open('creds.json'))\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = SignedJwtAssertionCredentials(json_key['client_email'], json_key['private_key'].encode(), scope)\n",
    "\n",
    "file = gspread.authorize(credentials)\n",
    "CD = file.open(\"Case Dispatcher 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Sheet_Names(GS):\n",
    "    \"\"\"Returns a list of all the sheet names in a Google spreadsheet.\"\"\"\n",
    "    names = []\n",
    "    for s in GS.worksheets():\n",
    "        sheet_name = re.findall(r\"'(.*?)'\",str(s))\n",
    "        names.append(''.join(sheet_name))\n",
    "    return names\n",
    "\n",
    "Sheet_names = Get_Sheet_Names(CD)\n",
    "\n",
    "# Convert each sheet into a dataframe and set first row as header\n",
    "\n",
    "#def Sheets_to_DFs(GSN,WS)\n",
    "d={}\n",
    "for sn in Sheet_names:\n",
    "    d['{0}_GS'.format(sn)]=pd.DataFrame(CD.worksheet(sn).get_all_values())\n",
    "for x, df in d.items():\n",
    "    df.columns = df.iloc[0]\n",
    "for x, df in d.items():  \n",
    "    df.drop(0, inplace=True)\n",
    "locals().update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases from CIFs to CD data\n",
    "\n",
    "New_Suspects = CIF_Suspects.iloc[:,[1,2,3,4,6]]\n",
    "New_Suspects.rename(columns = {\n",
    "    'full_name':'Name','phone_contact': 'Phone_Number(s)','cif_number': 'Case_ID'},\n",
    "           inplace=True)\n",
    "New_Police = New_Suspects\n",
    "New_Suspects = New_Suspects.reindex( columns = New_Suspects.columns.tolist() + list(Suspects_GS.columns))\n",
    "New_Suspects = New_Suspects.iloc[:,5:len(New_Suspects.columns)]\n",
    "Suspects = pd.concat([Suspects_GS,New_Suspects])\n",
    "Suspects = Suspects.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "New_Victims = CIF_Victims\n",
    "vcols = ['Case_ID','Name','Phone_Number(s)','Address','Victim_ID']\n",
    "New_Victims.columns = vcols\n",
    "New_Victims = New_Victims.reindex( columns = New_Victims.columns.tolist() + list(Victims_GS.columns))\n",
    "New_Victims = New_Victims.iloc[:,5:len(New_Victims.columns)]\n",
    "Victims = pd.concat([Victims_GS,New_Victims])\n",
    "Victims = Victims.drop_duplicates(subset='Victim_ID')\n",
    "\n",
    "New_Police.rename(columns = {'Name': 'Suspect_Name'})\n",
    "New_Police = New_Police.reindex( columns = New_Police.columns.tolist() + list(Police_GS.columns))\n",
    "New_Police = New_Police.iloc[:,5:len(New_Police.columns)]\n",
    "Police = pd.concat([Police_GS,New_Police])\n",
    "Police = Police.drop_duplicates(subset='Suspect_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move closed cases to closed sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Closed_Suspects = Suspects[(Suspects.Case_Status.str.contains(\"Closed\", na=False))]\n",
    "Closed_Victims = Victims[Victims.Case_Status.str.contains(\"Closed\", na=False)]\n",
    "Closed_Police = Police[Police.Case_Status.str.contains(\"Closed\", na=False)]\n",
    "\n",
    "Closed_Cases = [Closed_Suspects,Closed_Victims,Closed_Police]\n",
    "\n",
    "def add_cdate_var(Sheets):\n",
    "    \"\"\"Adds a variable with the current date to the end of each dataframe in a list.\"\"\"\n",
    "    today = date.today()\n",
    "    for sheet in Sheets:\n",
    "        if len(sheet)>0:\n",
    "            sheet.loc[:,'Date_Closed'] = today.strftime(\"%m/%d/%Y\")\n",
    "        else:\n",
    "            sheet['Date_Closed'] = \"\"\n",
    "\n",
    "add_cdate_var(Closed_Cases)\n",
    "\n",
    "Closed_Sus = pd.concat([Closed_Sus_GS,Closed_Suspects], sort=False)\n",
    "Closed_Pol = pd.concat([Closed_Pol_GS,Closed_Police], sort=False)\n",
    "Closed_Vic = pd.concat([Closed_Vic_GS,Closed_Victims], sort=False)\n",
    "\n",
    "# Next Step: Remove from Active Sheets\n",
    "Suspects = Suspects[~Suspects.Suspect_ID.isin(Closed_Suspects.Suspect_ID)]\n",
    "Police = Police[~Police.Suspect_ID.isin(Closed_Police.Suspect_ID)]\n",
    "Victims = Victims[~Victims.Victim_ID.isin(Closed_Victims.Victim_ID)]\n",
    "\n",
    "Closed_Suspects = Suspects[(Suspects.Suspect_ID.isin(Closed_Police.Suspect_ID)) |\n",
    "                            (~Suspects.Case_ID.isin(Victims.Case_ID))]\n",
    "Closed_Police = Police[(Police.Suspect_ID.isin(Closed_Suspects.Suspect_ID)) |\n",
    "                            (~Police.Case_ID.isin(Victims.Case_ID))]\n",
    "Closed_Victims = Victims[(~Victims.Case_ID.isin(Police.Case_ID)) |\n",
    "                           (~Victims.Case_ID.isin(Suspects.Case_ID))]\n",
    "\n",
    "Closed_Cases = [Closed_Suspects,Closed_Victims,Closed_Police]\n",
    "add_cdate_var(Closed_Cases)\n",
    "\n",
    "Closed_Sus = pd.concat([Closed_Sus,Closed_Suspects], sort=False).drop_duplicates(subset='Suspect_ID')\n",
    "Closed_Pol = pd.concat([Closed_Pol,Closed_Police], sort=False).drop_duplicates(subset='Suspect_ID')\n",
    "Closed_Vic = pd.concat([Closed_Vic,Closed_Victims], sort=False).drop_duplicates(subset='Victim_ID')\n",
    "\n",
    "Suspects = Suspects[~Suspects.Suspect_ID.isin(Closed_Suspects.Suspect_ID)]\n",
    "Police = Police[~Police.Suspect_ID.isin(Closed_Police.Suspect_ID)]\n",
    "Victims = Victims[~Victims.Victim_ID.isin(Closed_Victims.Victim_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Case Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Victims['Willing_to_Testify'] = Victims.Name[Victims.Case_Status.str.contains(\"Step Complete\", na=False)]\n",
    "\n",
    "Vics_Willing = Victims[pd.notnull(Victims['Willing_to_Testify'])]\n",
    "Vics_Willing = Vics_Willing.groupby(\n",
    "    'Case_ID',sort=False)['Willing_to_Testify'].apply(lambda x: ', '.join(x.astype(str)))\n",
    "\n",
    "Police = pd.merge(Police, Vics_Willing, how='left',on='Case_ID')\n",
    "Police.Victims_Willing_to_Testify = Police['Willing_to_Testify']\n",
    "Police.drop(columns=['Willing_to_Testify'], inplace=True)\n",
    "\n",
    "Weight_Victims_Willing = Parameters_GS.iloc[0,1]\n",
    "Weight_Bio_and_Location = Parameters_GS.iloc[1,1]\n",
    "Weight_Other_Suspects = Parameters_GS.iloc[2,1]\n",
    "Weight_Police_Willing = Parameters_GS.iloc[3,1]\n",
    "Weight_Recency_of_Case = Parameters_GS.iloc[4,1]\n",
    "\n",
    "Weight_Emience = Parameters_GS.iloc[0,5]\n",
    "Weight_Solvability = Parameters_GS.iloc[1,5]\n",
    "Weight_Strength_of_Case = Parameters_GS.iloc[2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organize Arrest data from Case Dispatcher\n",
    "\n",
    "Arrests = pd.DataFrame(Arrests_GS)\n",
    "Arrests.infer_objects()\n",
    "Arrests['Outcome (Arrest)'] = Arrests['Outcome (Arrest)'].fillna(0).astype(int)\n",
    "Arrests = Arrests.loc[Arrests['Outcome (Arrest)'] == 1]\n",
    "\n",
    "PBs = []\n",
    "for n in range(1,8):\n",
    "    PBs.append('PB' + str(n))\n",
    "    \n",
    "for PB in PBs:\n",
    "    Arrests[PB + '_ID'] = Arrests['IRF#'] + '.' + PB\n",
    "\n",
    "dpb={}\n",
    "for PB in PBs:\n",
    "    cnames = [col for col in Arrests.columns if PB in col]\n",
    "    dpb['df{0}'.format(PB)]=pd.DataFrame(Arrests[cnames])\n",
    "newcn = ['Name','Arrested','Arrest_Date','PB_ID']\n",
    "locals().update(dpb)\n",
    "\n",
    "df_list = [dfPB1,dfPB2,dfPB3,dfPB4,dfPB5,dfPB6,dfPB7]\n",
    "for i, df in enumerate(df_list, 1):\n",
    "    df.columns = ['Name','Arrested','Arrest_Date','PB_ID']\n",
    "    \n",
    "dfPBAll = pd.concat(df_list)\n",
    "\n",
    "Arrests = dfPBAll[dfPBAll['Arrested'].str.contains(\"Yes\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36_32)",
   "language": "python",
   "name": "py36_32"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
