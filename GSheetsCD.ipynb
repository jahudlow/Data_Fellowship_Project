{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from config import config\n",
    "import db_module as dm\n",
    "import gsheets as gs\n",
    "from closed_date import add_cdate_var\n",
    "import gspread\n",
    "from oauth2client.client import SignedJwtAssertionCredentials\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import zipfile\n",
    "import re\n",
    "from datetime import date\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant tables from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_mudule.py\n",
    "#\n",
    "#from config import config\n",
    "#import pandas as pd\n",
    "#import psycopg2\n",
    "#\n",
    "#class DBConn(object):\n",
    "#    '''This is a class for querying the database and returning a pandas dataframe.'''\n",
    "#    def __init__(self):\n",
    "#        params = config()\n",
    "#        conn = psycopg2.connect(**params)\n",
    "#        self.cur = conn.cursor()\n",
    "#    def ex_query(self, query):\n",
    "#        query = query\n",
    "#        cur = self.cur\n",
    "#        cur.execute(query)\n",
    "#        colnames = [desc[0] for desc in cur.description]\n",
    "#        rows = cur.fetchall()\n",
    "#        cur.close()\n",
    "#        return pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "cif = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM dataentry_cifnepal as CIF inner join \\\n",
    "    dataentry_personboxnepal as PB on CIF.id = PB.cif_id;\")\n",
    "db_cif = cif.ex_query(query)\n",
    "\n",
    "vics = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM public.dataentry_person as p inner join \\\n",
    "    dataentry_cifnepal as CIF on p.id = CIF.main_pv_id;\")\n",
    "db_vics = vics.ex_query(query)\n",
    "\n",
    "sus = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM public.dataentry_personboxnepal as pb inner join \\\n",
    "    public.dataentry_person as p on pb.person_id = p.id;\")\n",
    "db_sus = sus.ex_query(query)\n",
    "\n",
    "add = dm.DBConn()\n",
    "query = str(\n",
    "    \"SELECT * FROM public.dataentry_address1 as ad1 inner join \\\n",
    "    public.dataentry_address2 as ad2 on ad1.id = ad2.address1_id;\")\n",
    "db_add = add.ex_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data from CIFs and create suspect and victim IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cif_dates = dfcif[['cif_number','interview_date']]\n",
    "dfcif = dfcif[['cif_number','person_id','pb_number']]\n",

    "\n",
    "add = db_add.iloc[:,[1,6,7]]\n",
    "acols = ['address_1',\n",
    "         'address2_id',\n",
    "         'address_2']\n",
    "add.columns = acols\n",
    "\n",
    "db_vics.infer_objects\n",
    "db_vics['address1_id'] = db_vics['address1_id'].fillna(0).astype(int)\n",
    "db_vics['address2_id'] = db_vics['address2_id'].fillna(0).astype(int)\n",
    "db_vics = pd.merge(db_vics, add, how='left',on='address2_id')\n",
    "db_vics['Address'] = db_vics['address_2'].map(str) + \", \" + db_vics['address_1']\n",
    "cif_vics = db_vics[['cif_number',\n",
    "                    'full_name',\n",
    "                    'phone_contact',\n",
    "                    'Address']]\n",
    "\n",
    "db_sus.infer_objects\n",
    "db_sus['address1_id'] = db_sus['address1_id'].fillna(0).astype(int)\n",
    "db_sus['address2_id'] = db_sus['address2_id'].fillna(0).astype(int)\n",
    "db_sus = pd.merge(db_sus, add, how='left', on='address2_id')\n",
    "db_sus['Address'] = db_sus['address_2'].map(str) + \", \" + db_sus['address_1']\n",
    "db_sus = db_sus[['person_id', 'full_name', 'phone_contact', 'Address']]\n",
    "cif_sus = pd.merge(db_sus, cif_ids, how='outer', on='person_id', sort=True,\n",
    "         suffixes=('x', 'y'), copy=True)\n",
    "cif_sus.loc[:,'pb_number']=cif_sus['pb_number'].fillna(0).astype(int)\n",
    "cif_sus.loc[:,'Suspect_ID'] = cif_sus.loc[:,'cif_number'].str.replace('.','')\n",
    "cif_sus.loc[:,'Suspect_ID'] = cif_sus.loc[:,'Suspect_ID'].str[:-1] + \".PB\" + cif_sus['pb_number'].map(str)\n",
    "cif_sus = cif_sus.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "cif_vics.loc[:, 'Victim_ID'] = cif_vics['cif_number']\n",
    "replacements = {\n",
    "   'Victim_ID': {\n",
    "      r'(\\.1|A$)': '.V1',r'B$': '.V2',r'C$': '.V3', r'D$': '.V4',r'E$': '.V5',\n",
    "      r'F$': '.V6',r'G$': '.V7',r'H$': '.V8',r'I$': '.V9',r'J$': '.V10'}\n",
    "}\n",
    "cif_vics.replace(replacements, regex=True, inplace=True)\n",
    "cif_vics.sort_values('full_name',inplace=True)\n",
    "cif_vics = cif_vics.drop_duplicates(subset='Victim_ID')\n",
    "non_blanks = cif_vics['full_name'] != \"\"\n",
    "cif_vics = cif_vics[non_blanks]\n",
    "\n",
    "cif_sus.loc[:, 'cif_number'] = cif_sus['cif_number'].str.replace('.','')\n",
    "cif_vics['cif_number'] = cif_vics['cif_number'].str.replace('.','')\n",
    "cif_vics['cif_number'] = cif_vics['cif_number'].str[:-1]\n",
    "cif_sus['cif_number'] = cif_sus['cif_number'].str[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get current Case Dispatcher data from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting latest CD data from Google Sheets\n",
    "\n",
    "json_key = json.load(open('creds.json'))\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = SignedJwtAssertionCredentials(json_key['client_email'], json_key['private_key'].encode(), scope)\n",
    "\n",
    "file = gspread.authorize(credentials)\n",
    "CD = file.open(\"Case Dispatcher 2.0\")\n",
    "\n",
    "def Get_Sheet_Names(GS):\n",
    "    \"\"\"Returns a list of all the sheet names in a Google spreadsheet.\"\"\"\n",
    "    names = []\n",
    "    for s in GS.worksheets():\n",
    "        sheet_name = re.findall(r\"'(.*?)'\",str(s))\n",
    "        names.append(''.join(sheet_name))\n",
    "    return names\n",
    "\n",
    "Sheet_names = Get_Sheet_Names(CD)\n",
    "\n",
    "# Convert each sheet into a dataframe and set first row as header\n",
    "\n",
    "#def Sheets_to_DFs(GSN,WS)\n",
    "d={}\n",
    "for sn in Sheet_names:\n",
    "    d['{0}_GS'.format(sn)]=pd.DataFrame(CD.worksheet(sn).get_all_values())\n",
    "for x, df in d.items():\n",
    "    df.columns = df.iloc[0]\n",
    "for x, df in d.items():  \n",
    "    df.drop(0, inplace=True)\n",
    "locals().update(d)"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,

   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases from CIFs to CD data\n",
    "\n",
    "new_suspects = cif_sus.iloc[:,[1, 2, 3, 4, 6]]\n",
    "new_suspects.rename(columns = {\n",
    "    'full_name':'Name',\n",
    "    'phone_contact': 'Phone_Number(s)',\n",
    "    'cif_number': 'Case_ID'},\n",
    "           inplace=True)\n",
    "New_Police = New_Suspects\n",
    "New_Suspects = New_Suspects.reindex( columns = New_Suspects.columns.tolist() + list(Suspects_GS.columns))\n",
    "New_Suspects = New_Suspects.iloc[:,5:len(New_Suspects.columns)]\n",
    "Suspects = pd.concat([Suspects_GS,New_Suspects])\n",
    "Suspects = Suspects.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "New_Victims = CIF_Victims\n",
    "vcols = ['Case_ID','Name','Phone_Number(s)','Address','Victim_ID']\n",
    "New_Victims.columns = vcols\n",
    "New_Victims = New_Victims.reindex( columns = New_Victims.columns.tolist() + list(Victims_GS.columns))\n",
    "New_Victims = New_Victims.iloc[:,5:len(New_Victims.columns)]\n",
    "Victims = pd.concat([Victims_GS,New_Victims])\n",
    "Victims = Victims.drop_duplicates(subset='Victim_ID')\n",
    "\n",
    "New_Police.rename(columns = {'Name': 'Suspect_Name'})\n",
    "New_Police = New_Police.reindex( columns = New_Police.columns.tolist() + list(Police_GS.columns))\n",
    "New_Police = New_Police.iloc[:,5:len(New_Police.columns)]\n",
    "Police = pd.concat([Police_GS,New_Police])\n",
    "Police = Police.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "\n",
    "## Organize Arrest data from Case Dispatcher\n",
    "\n",
    "Arrests = pd.DataFrame(Arrests_GS)\n",
    "Arrests.infer_objects()\n",
    "Arrests['Outcome (Arrest)'] = Arrests['Outcome (Arrest)'].fillna(0).astype(int)\n",
    "Arrests = Arrests.loc[Arrests['Outcome (Arrest)'] == 1]\n",
    "\n",
    "PBs = ['PB' + str(n) for n in range(1,8)]\n",
    "for PB in PBs:\n",
    "    Arrests[PB + '_ID'] = Arrests['IRF#'] + '.' + PB\n",
    "for PB in PBs:\n",
    "    Arrests[PB + '_Case_ID'] = Arrests['IRF#']\n",
    "\n",
    "dpb={}\n",
    "for PB in PBs:\n",
    "    cnames = [col for col in Arrests.columns if PB in col]\n",
    "    dpb['df{0}'.format(PB)]=pd.DataFrame(Arrests[cnames])\n",
    "newcn = ['Name','Arrested','Arrest_Date','Suspect_ID','Case_ID']\n",
    "locals().update(dpb)\n",
    "\n",
    "df_list = [dfPB1,dfPB2,dfPB3,dfPB4,dfPB5,dfPB6,dfPB7]\n",
    "for i, df in enumerate(df_list, 1):\n",
    "    df.columns = ['Name','Arrested','Arrest_Date','Suspect_ID','Case_ID']\n",
    "    \n",
    "dfPBAll = pd.concat(df_list)\n",
    "\n",
    "Arrests = dfPBAll[dfPBAll.Arrested.str.contains(\"Yes\")]\n",
    "Arrests['Total_Arrests'] = Arrests.groupby(['Case_ID'])['Case_ID'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move closed cases to closed sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prev_Closed_Sus = Suspects[Suspects.Suspect_ID.isin(Arrests.Suspect_ID)]\n",
    "Prev_Closed_Pol = Police[Police.Suspect_ID.isin(Arrests.Suspect_ID)]\n",
    "Prev_Closed_Sus['Case_Status'] = \"Closed: Already in Legal Cases Sheet\"\n",
    "Prev_Closed_Pol['Case_Status'] = \"Closed: Already in Legal Cases Sheet\"\n",
    "\n",
    "Closed_Suspects = Suspects[(Suspects.Case_Status.str.contains(\"Closed\", na=False))]\n",
    "Closed_Victims = Victims[Victims.Case_Status.str.contains(\"Closed\", na=False)]\n",
    "Closed_Police = Police[Police.Case_Status.str.contains(\"Closed\", na=False)]\n",
    "\n",
    "Closed_Cases = [Closed_Suspects,Closed_Victims,Closed_Police, Prev_Closed_Sus, Prev_Closed_Pol]\n",
    "\n",
    "def add_cdate_var(Sheets):\n",
    "    \"\"\"Adds a variable with the current date to the end of each dataframe in a list.\"\"\"\n",
    "    today = date.today()\n",
    "    for sheet in Sheets:\n",
    "        if len(sheet)>0:\n",
    "            sheet.loc[:,'Date_Closed'] = today.strftime(\"%m/%d/%Y\")\n",
    "        else:\n",
    "            sheet['Date_Closed'] = \"\"\n",
    "\n",
    "add_cdate_var(Closed_Cases)\n",
    "\n",
    "Closed_Sus = pd.concat([Closed_Sus_GS,Closed_Suspects, Prev_Closed_Sus], sort=False)\n",
    "Closed_Pol = pd.concat([Closed_Pol_GS,Closed_Police, Prev_Closed_Pol], sort=False)\n",
    "Closed_Vic = pd.concat([Closed_Vic_GS,Closed_Victims], sort=False)\n",
    "\n",
    "# Next Step: Remove from Active Sheets\n",
    "Suspects = Suspects[~Suspects.Suspect_ID.isin(Closed_Suspects.Suspect_ID)]\n",
    "Police = Police[~Police.Suspect_ID.isin(Closed_Police.Suspect_ID)]\n",
    "Victims = Victims[~Victims.Victim_ID.isin(Closed_Victims.Victim_ID)]\n",
    "\n",
    "Closed_Suspects = Suspects[(Suspects.Suspect_ID.isin(Closed_Police.Suspect_ID)) |\n",
    "                            (~Suspects.Case_ID.isin(Victims.Case_ID))]\n",
    "Closed_Police = Police[(Police.Suspect_ID.isin(Closed_Suspects.Suspect_ID)) |\n",
    "                            (~Police.Case_ID.isin(Victims.Case_ID))]\n",
    "Closed_Victims = Victims[(~Victims.Case_ID.isin(Police.Case_ID)) |\n",
    "                           (~Victims.Case_ID.isin(Suspects.Case_ID))]\n",
    "\n",
    "Closed_Cases = [Closed_Suspects,Closed_Victims,Closed_Police]\n",
    "add_cdate_var(Closed_Cases)\n",
    "\n",
    "Closed_Sus = pd.concat([Closed_Sus,Closed_Suspects], sort=False).drop_duplicates(subset='Suspect_ID')\n",
    "Closed_Pol = pd.concat([Closed_Pol,Closed_Police], sort=False).drop_duplicates(subset='Suspect_ID')\n",
    "Closed_Vic = pd.concat([Closed_Vic,Closed_Victims], sort=False).drop_duplicates(subset='Victim_ID')\n",
    "\n",
    "Suspects = Suspects[~Suspects.Suspect_ID.isin(Closed_Sus.Suspect_ID)]\n",
    "Police = Police[~Police.Suspect_ID.isin(Closed_Pol.Suspect_ID)]\n",
    "Victims = Victims[~Victims.Victim_ID.isin(Closed_Vic.Victim_ID)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Case Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,

   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RandA\\Anaconda3\\envs\\py36_32\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "Victims['Willing_to_Testify'] = Victims.Name[Victims.Case_Status.str.contains(\"Step Complete\", na=False)]\n",
    "Vics_Willing = Victims[['Case_ID','Willing_to_Testify']]\n",
    "Vics_Willing = Vics_Willing.dropna(axis=0, subset=['Willing_to_Testify'])\n",
    "Vics_Willing['Count'] = 1\n",
    "def sum_and_join(x):\n",
    "     return pd.Series(dict(Count = x['Count'].sum(), \n",
    "                        Willing_to_Testify = ', '.join(x.astype(str)['Willing_to_Testify'])))\n",
    "if len(Vics_Willing) > 0:\n",
    "    Vics_Willing = Vics_Willing.groupby('Case_ID').apply(sum_and_join)\n",
    "    \n",
    "Police = pd.merge(Police, Vics_Willing, how='left',on='Case_ID')\n",
    "\n",
    "Police.Victims_Willing_to_Testify = Police.Willing_to_Testify\n",
    "Police.drop(columns=['Willing_to_Testify', 'Count'], inplace=True)\n",
    "\n",
    "Suspects = pd.merge(Suspects, Vics_Willing, how='left',on='Case_ID')\n",
    "V_Multiplier = pd.DataFrame(Parameters_GS.iloc[:10,6:8])\n",
    "V_Multiplier.Victims_Willing_to_Testify = V_Multiplier.Victims_Willing_to_Testify.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Suspects['Count'] = Suspects['Count'].fillna(0).astype(int)\n",
    "Suspects = pd.merge(Suspects, V_Multiplier,how='left',left_on='Count',right_on='Victims_Willing_to_Testify')\n",
    "Suspects.drop(columns=['Victims_Willing_to_Testify','Willing_to_Testify','Count'], inplace=True)\n",
    "Suspects['V_Multiplier'].fillna(0, inplace=True)\n",
    "Suspects['V_Multiplier'] = Suspects['V_Multiplier'].astype('float')\n",
    "Suspects['Bio_Known'] = np.where(Suspects['Bio_and_Location'].eq(''),0,1)\n",
    "Suspects = pd.merge(Suspects, Arrests[['Case_ID','Total_Arrests']], how='left',on='Case_ID')\n",
    "Suspects['Total_Arrests'] = Suspects['Total_Arrests'].fillna(0).astype(int)\n",
    "Suspects.rename(columns={'Total_Arrests':'Others_Arrested'}, inplace=True)\n",
    "Police['Willing_to_Arrest'] = np.where(Police.Case_Status.str.contains(\"Step Complete\", na=False),1,0)\n",
    "Suspects = pd.merge(Suspects,Police[['Case_ID','Willing_to_Arrest']], how='left', on='Case_ID')\n",
    "today = date.today()\n",
    "today.strftime(\"%m/%d/%Y\")\n",
    "cif_dates['Case_ID'] = cif_dates['cif_number'].str[:-1].replace('.','')\n",
    "cif_dates['Days_Old'] = (today - cif_dates.loc[:,'interview_date']) / np.timedelta64(1, 'D')\n",
    "Suspects = pd.merge(Suspects,cif_dates[['Case_ID','Days_Old']], how='left', on='Case_ID')\n",
    "Suspects['Recency_Score'] = np.where(Suspects['Days_Old']<100, 1 - Suspects.Days_Old * .01, 0)\n",
    "Suspects = Suspects.drop_duplicates(subset='Suspect_ID')\n",
    "\n",
    "# Get 'Strength of Case' results of CD module\n",
    "SOC = pd.read_csv(\"SOC.csv\")\n",
    "Suspects = pd.merge(Suspects,SOC,how='left',left_on='Suspect_ID',right_on='suspect_id')\n",
    "Suspects['Strength_of_Case'] = Suspects['SOC'].round(decimals = 3)\n",
    "Suspects['Strength_of_Case']\n",
    "\n",
    "Suspects['Em2'] = Suspects['Eminence'].fillna(1)\n",
    "Suspects.loc[Suspects['Eminence'].str.len() < 1, 'Em2'] = 1\n",
    "Suspects['Em2'] = Suspects['Em2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights_Vs = pd.Series(Parameters_GS.iloc[0:7,1]).replace('',0).append(pd.Series(Parameters_GS.iloc[0:3,5])).astype(float)\n",
    "Weights_Keys = pd.Series(Parameters_GS.iloc[0:7,0]).append(pd.Series(Parameters_GS.iloc[0:3,4]))\n",
    "Weights = {k:v for k,v in zip(Weights_Keys, Weights_Vs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Suspects['Solvability'] = (\n",
    "    Suspects['V_Multiplier'].apply(lambda x: x * Weights['Victim Willing to Testify']) + \\\n",
    "    Suspects['Bio_Known'].apply(lambda x: x * Weights['Bio and Location of Suspect']) + \\\n",
    "    Suspects['Others_Arrested'].apply(lambda x: x * Weights['Other Suspect(s) Arrested']) + \\\n",
    "    Suspects['Willing_to_Arrest'].apply(lambda x: x * Weights['Police Willing to Arrest']) + \\\n",
    "    Suspects['Recency_Score'].apply(lambda x: x * Weights['Recency of Case'])\n",
    ")/sum(Weights.values())\n",
    "\n",
    "Suspects['Priority'] = (\n",
    "    Suspects['Solvability'].apply(lambda x: x * Weights['Solvability']) + \\\n",
    "    Suspects['Strength_of_Case'].apply(lambda x: x * Weights['Strength of Case']) + \\\n",
    "    Suspects['Em2'].apply(lambda x: x * 0.1 * Weights['Eminence'])\n",
    "    ).round(decimals = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload results to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Suspects_Final = Suspects.iloc[:,0:41].fillna('')\n",
    "Suspects_Final.to_csv('Suspects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = gspread.authorize(credentials)\n",
    "CD = file.open(\"Case Dispatcher 2.0\")\n",
    "\n",
    "Suspects_Final = open('Suspects.csv', 'r').read()\n",
    "\n",
    "file.import_csv(CD.id, Suspects_Final) \n",
    "# This overwrites entire spreadsheet, need to find a way to import into one worksheet tab instead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36_32)",
   "language": "python",
   "name": "py36_32"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
